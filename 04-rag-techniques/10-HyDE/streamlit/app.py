import streamlit as st
import numpy as np
import fitz  # PyMuPDF
import os
import io
import config
from openai import OpenAI
from dotenv import load_dotenv
from PIL import Image

# --- 1. ê¸°ë³¸ ì„¤ì • ë° API í‚¤ ë¡œë“œ ---

st.set_page_config(
    page_title="HyDE RAG ë¬¸ì„œ ë¶„ì„ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
try:
    client = OpenAI(api_key=config.API_KEY)
    #client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    st.error("OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”. .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()


# --- 2. Jupyter Notebookì˜ í•µì‹¬ ë¡œì§ (í´ë˜ìŠ¤ ë° í•¨ìˆ˜) ---
# SimpleVectorStore, ë¬¸ì„œ ì²˜ë¦¬, ì„ë² ë”©, RAG í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

class SimpleVectorStore:
    """NumPyë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë²¡í„° ì €ì¥ì†Œ êµ¬í˜„ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    def __init__(self):
        self.vectors = []
        self.texts = []
        self.metadata = []

    def add_item(self, text, embedding, metadata=None):
        self.vectors.append(np.array(embedding))
        self.texts.append(text)
        self.metadata.append(metadata or {})

    def similarity_search(self, query_embedding, k=5):
        if not self.vectors:
            return []
        query_vector = np.array(query_embedding)
        similarities = []
        for i, vector in enumerate(self.vectors):
            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))
            similarities.append((i, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for i in range(min(k, len(similarities))):
            idx, score = similarities[i]
            results.append({
                "text": self.texts[idx],
                "metadata": self.metadata[idx],
                "similarity": float(score)
            })
        return results

def extract_text_from_pdf(pdf_bytes):
    """PDF íŒŒì¼ì˜ ë°”ì´íŠ¸ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    pages = []
    with fitz.open(stream=pdf_bytes, filetype="pdf") as pdf:
        for page_num, page in enumerate(pdf):
            text = page.get_text()
            if len(text.strip()) > 50:
                pages.append({
                    "text": text,
                    "metadata": {"page": page_num + 1}
                })
    return pages

def describe_image(image_bytes, filename):
    """OpenAI Vision API(gpt-4o)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "ì´ ì´ë¯¸ì§€ëŠ” ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ ëª¨ë“  ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸, ë‹¤ì´ì–´ê·¸ë¨, ì°¨íŠ¸, ì£¼ìš” ê°ì²´, ì „ì²´ì ì¸ ë§¥ë½ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_bytes}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=1024
        )
        description = response.choices[0].message.content
        return {
            "text": description,
            "metadata": {"source_image": filename}
        }
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {filename}, ì˜¤ë¥˜: {e}")
        return None

def chunk_text(text, chunk_size=1000, overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ ì¼ì • ê¸¸ì´ì˜ ì¤‘ì²© ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk_text = text[i:i + chunk_size]
        if chunk_text:
            chunks.append({
                "text": chunk_text,
                "metadata": {"start_pos": i, "end_pos": i + len(chunk_text)}
            })
    return chunks

def create_embeddings(texts, model="text-embedding-3-small"):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not texts:
        return []
    try:
        response = client.embeddings.create(model=model, input=texts)
        return [item.embedding for item in response.data]
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def generate_hypothetical_document(query, model="gpt-4o-mini"):
    """ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ê°€ìƒì˜ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ ë¬¸ì„œ ì‘ì„±ìì…ë‹ˆë‹¤. 
    ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì´ ë  ê°€ìƒì˜ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    ì´ ë¬¸ì„œëŠ” ì£¼ì œì— ëŒ€í•´ ê¹Šì´ ìˆê³  ì •ë³´ê°€ í’ë¶€í•œ ì„¤ëª…ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    ì‹¤ì œ ë¬¸ì„œì²˜ëŸ¼ ì‚¬ì‹¤, ì˜ˆì‹œ, ê°œë… ë“±ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”."""
    user_prompt = f"ì§ˆë¬¸: {query}\n\nì´ ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”:"
    
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.1
    )
    return response.choices[0].message.content

def generate_response(query, relevant_chunks, model="gpt-4o-mini"):
    """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    context = "\n\n".join([chunk["text"] for chunk in relevant_chunks])
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "user", "content": f"ë¬¸ë§¥:\n{context}\n\nì§ˆë¬¸: {query}"}
        ],
        temperature=0.5,
        max_tokens=1000
    )
    return response.choices[0].message.content

# --- 3. Streamlit UI ë° ìƒíƒœ ê´€ë¦¬ ---

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processed" not in st.session_state:
    st.session_state.processed = False

# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.title("ğŸ“„ HyDE RAG ì±—ë´‡ ì„¤ì •")
    st.markdown("---")
    
    uploaded_files = st.file_uploader(
        "PDF ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['pdf', 'png', 'jpg', 'jpeg'],
        accept_multiple_files=True
    )

    st.markdown("---")
    st.subheader("RAG ì„¤ì •")
    rag_mode = st.radio(
        "RAG ë°©ì‹ ì„ íƒ",
        ("Standard RAG", "HyDE RAG", "Compare Both"),
        index=2, # ê¸°ë³¸ê°’ 'Compare Both'
        help="Standard: ì§ˆë¬¸ì„ ì§ì ‘ ì‚¬ìš©. HyDE: ì§ˆë¬¸ìœ¼ë¡œ ê°€ìƒ ë¬¸ì„œë¥¼ ë§Œë“¤ì–´ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ. Compare: ë‘ ë°©ì‹ ë¹„êµ."
    )
    
    k_chunks = st.slider("ê²€ìƒ‰í•  ì²­í¬ ìˆ˜ (k)", 1, 10, 5)

    if st.button("íŒŒì¼ ì²˜ë¦¬ ë° ì§€ì‹ ê¸°ë°˜ ìƒì„±"):
        if not uploaded_files:
            st.warning("ë¶„ì„í•  íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("íŒŒì¼ ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                all_processed_texts = []
                for uploaded_file in uploaded_files:
                    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
                    if uploaded_file.type == "application/pdf":
                        pdf_bytes = uploaded_file.getvalue()
                        pages = extract_text_from_pdf(pdf_bytes)
                        for page in pages:
                            page["metadata"]["source"] = uploaded_file.name
                            all_processed_texts.append(page)
                    else: # ì´ë¯¸ì§€ íŒŒì¼
                        image_bytes = uploaded_file.getvalue()
                        image_base64 = io.BytesIO(image_bytes).read()
                        import base64
                        encoded_image = base64.b64encode(image_base64).decode('utf-8')
                        desc_obj = describe_image(encoded_image, uploaded_file.name)
                        if desc_obj:
                            all_processed_texts.append(desc_obj)
                
                # í…ìŠ¤íŠ¸ ì²­í‚¹
                all_chunks_with_meta = []
                for item in all_processed_texts:
                    page_chunks = chunk_text(item["text"])
                    for chunk in page_chunks:
                        chunk["metadata"].update(item["metadata"])
                    all_chunks_with_meta.extend(page_chunks)

                # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                chunk_texts_only = [chunk["text"] for chunk in all_chunks_with_meta]
                chunk_embeddings = create_embeddings(chunk_texts_only)
                
                if chunk_embeddings:
                    vector_store = SimpleVectorStore()
                    for i, chunk in enumerate(all_chunks_with_meta):
                        vector_store.add_item(
                            text=chunk["text"],
                            embedding=chunk_embeddings[i],
                            metadata=chunk["metadata"]
                        )
                    st.session_state.vector_store = vector_store
                    st.session_state.processed = True
                    st.session_state.messages = [] # ìƒˆ íŒŒì¼ ì²˜ë¦¬ ì‹œ ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
                    st.success(f"{len(uploaded_files)}ê°œ íŒŒì¼ì—ì„œ ì´ {len(all_chunks_with_meta)}ê°œì˜ ì •ë³´ ì¡°ê°ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.markdown("---")
    st.info("ë§Œë“ ì´: AI Assistant\n- Jupyter Notebookì˜ HyDE RAG ë¡œì§ì„ Streamlit ì•±ìœ¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.")

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
st.title("HyDE RAG ì±—ë´‡")
st.markdown("ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'íŒŒì¼ ì²˜ë¦¬' ë²„íŠ¼ì„ ëˆ„ë¥¸ í›„, ì•„ë˜ ì±„íŒ…ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if query := st.chat_input("ì—…ë¡œë“œí•œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    if not st.session_state.processed:
        st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
        st.chat_message("user").markdown(query)
        st.session_state.messages.append({"role": "user", "content": query})

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            # ë¡œë”© ìŠ¤í”¼ë„ˆ
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                
                def run_standard_rag(q, vs, k):
                    query_embedding = create_embeddings([q])[0]
                    chunks = vs.similarity_search(query_embedding, k=k)
                    response = generate_response(q, chunks)
                    return response, chunks

                def run_hyde_rag(q, vs, k):
                    hypo_doc = generate_hypothetical_document(q)
                    hypo_embedding = create_embeddings([hypo_doc])[0]
                    chunks = vs.similarity_search(hypo_embedding, k=k)
                    response = generate_response(q, chunks)
                    return response, chunks, hypo_doc

                # ì„ íƒëœ ëª¨ë“œì— ë”°ë¼ RAG ì‹¤í–‰
                if rag_mode == "Standard RAG":
                    response, chunks = run_standard_rag(query, st.session_state.vector_store, k_chunks)
                    st.markdown(response)
                    with st.expander("ì°¸ê³ í•œ ì •ë³´ ì¡°ê° (Standard RAG)"):
                        for chunk in chunks:
                            st.info(f"**ìœ ì‚¬ë„: {chunk['similarity']:.4f}** (ì¶œì²˜: {chunk['metadata'].get('source', 'N/A')}, í˜ì´ì§€: {chunk['metadata'].get('page', 'N/A')})\n\n> {chunk['text']}")
                    st.session_state.messages.append({"role": "assistant", "content": response})

                elif rag_mode == "HyDE RAG":
                    response, chunks, hypo_doc = run_hyde_rag(query, st.session_state.vector_store, k_chunks)
                    st.markdown(response)
                    with st.expander("ì°¸ê³ í•œ ì •ë³´ ì¡°ê° (HyDE RAG)"):
                        for chunk in chunks:
                            st.info(f"**ìœ ì‚¬ë„: {chunk['similarity']:.4f}** (ì¶œì²˜: {chunk['metadata'].get('source', 'N/A')}, í˜ì´ì§€: {chunk['metadata'].get('page', 'N/A')})\n\n> {chunk['text']}")
                    with st.expander("HyDEê°€ ìƒì„±í•œ ê°€ìƒ ë¬¸ì„œ"):
                        st.warning(hypo_doc)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                
                else: # Compare Both
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("Standard RAG ê²°ê³¼")
                        std_response, std_chunks = run_standard_rag(query, st.session_state.vector_store, k_chunks)
                        st.markdown(std_response)
                        with st.expander("ì°¸ê³ í•œ ì •ë³´ ì¡°ê° (Standard)"):
                            for chunk in std_chunks:
                                st.info(f"**ìœ ì‚¬ë„: {chunk['similarity']:.4f}**\n\n> {chunk['text']}")
                    
                    with col2:
                        st.subheader("HyDE RAG ê²°ê³¼")
                        hyde_response, hyde_chunks, hypo_doc = run_hyde_rag(query, st.session_state.vector_store, k_chunks)
                        st.markdown(hyde_response)
                        with st.expander("ì°¸ê³ í•œ ì •ë³´ ì¡°ê° (HyDE)"):
                            for chunk in hyde_chunks:
                                st.info(f"**ìœ ì‚¬ë„: {chunk['similarity']:.4f}**\n\n> {chunk['text']}")
                        with st.expander("HyDE ìƒì„± ê°€ìƒ ë¬¸ì„œ"):
                            st.warning(hypo_doc)
                    
                    # ë¹„êµ ëª¨ë“œì—ì„œëŠ” ì±„íŒ… ê¸°ë¡ì— ë‘ ë‹µë³€ì„ ëª¨ë‘ ì¶”ê°€
                    comparison_response = f"**Standard RAG ë‹µë³€:**\n{std_response}\n\n---\n\n**HyDE RAG ë‹µë³€:**\n{hyde_response}"
                    st.session_state.messages.append({"role": "assistant", "content": comparison_response})