{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Adaptive Retrieval for Enhanced RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시나리오 예시\n",
    "\n",
    "질문: “퇴사 이후 연차 정산은 어떻게 이루어지나요?”\n",
    "\n",
    "→ 분류: 문맥 기반 질문\n",
    "\n",
    "→ 전략: Step-back Prompting + Contextual Chunking\n",
    "\n",
    "→ 검색: \"퇴직 시 근로기준법 적용\", \"연차 정산 조건\", \"예외 조항\" 포함 섹션\n",
    "\n",
    "→ 응답: 문맥 흐름을 반영하여 정리된 맞춤형 설명 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        str: PDF에서 추출된 전체 텍스트\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 전체 텍스트를 저장할 문자열 초기화\n",
    "\n",
    "    # 각 페이지를 순회하며 텍스트 추출\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]               # 해당 페이지 가져오기\n",
    "        text = page.get_text(\"text\")         # 텍스트 형식으로 내용 추출\n",
    "        all_text += text                     # 추출된 텍스트 누적\n",
    "\n",
    "    # 추출된 전체 텍스트 반환\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로 분할하되, 각 청크 간에 overlap만큼 겹치게 합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        n (int): 각 청크의 문자 수 (기본값: 1000)\n",
    "        overlap (int): 청크 간 겹치는 문자 수 (기본값: 200)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 분할된 텍스트 청크 리스트\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크들을 저장할 리스트 초기화\n",
    "\n",
    "    # n - overlap 만큼 이동하면서 청크 생성\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # i에서 i + n까지의 텍스트 조각을 청크로 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 생성된 청크 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 활용한 간단한 벡터 저장소 구현체입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []     # 임베딩 벡터 리스트\n",
    "        self.texts = []       # 원본 텍스트 리스트\n",
    "        self.metadata = []    # 각 텍스트의 메타데이터 리스트\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        단일 텍스트 항목을 벡터 저장소에 추가합니다.\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "            embedding (List[float]): 텍스트의 임베딩 벡터\n",
    "            metadata (dict, optional): 추가 메타데이터 (기본값: 빈 딕셔너리)\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 NumPy 배열로 변환하여 저장\n",
    "        self.texts.append(text)                   # 원본 텍스트 저장\n",
    "        self.metadata.append(metadata or {})      # 메타데이터 저장 (None일 경우 빈 딕셔너리)\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 텍스트를 검색합니다.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 질의 임베딩 벡터\n",
    "            k (int): 반환할 상위 결과 개수\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 유사한 항목 리스트 (텍스트, 메타데이터, 유사도 포함)\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "\n",
    "        # 질의 벡터를 NumPy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        # 각 벡터와의 코사인 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (\n",
    "                np.linalg.norm(query_vector) * np.linalg.norm(vector)\n",
    "            )\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 저장\n",
    "\n",
    "        # 유사도 기준 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 상위 k개 항목 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대해 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트(또는 텍스트 리스트)\n",
    "        model (str): 사용할 임베딩 모델 이름\n",
    "\n",
    "    Returns:\n",
    "        List[float] 또는 List[List[float]]: 생성된 임베딩 벡터 또는 벡터 리스트\n",
    "    \"\"\"\n",
    "    # 입력이 문자열 하나일 수도 있고, 문자열 리스트일 수도 있으므로 리스트 형태로 통일\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "\n",
    "    # 지정된 모델을 사용하여 임베딩 생성 요청\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "\n",
    "    # 입력이 단일 문자열이었을 경우, 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    # 여러 문자열일 경우, 모든 임베딩 리스트 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    적응형 검색을 위한 문서 처리 함수\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): 처리할 PDF 파일 경로\n",
    "        chunk_size (int): 각 청크의 문자 수\n",
    "        chunk_overlap (int): 청크 간 겹치는 문자 수\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore]: 텍스트 청크 리스트와 벡터 저장소\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 텍스트를 일정 길이로 청크 분할\n",
    "    print(\"텍스트를 청크 단위로 분할 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"{len(chunks)}개의 텍스트 청크 생성 완료\")\n",
    "\n",
    "    # 각 청크에 대해 임베딩 생성\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "\n",
    "    # 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # 각 청크와 임베딩, 메타데이터를 저장소에 추가\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "\n",
    "    print(f\"총 {len(chunks)}개의 청크가 벡터 저장소에 추가됨\")\n",
    "\n",
    "    # 텍스트 청크와 벡터 저장소 반환\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_query(query, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    사용자의 질의를 다음 네 가지 중 하나로 분류합니다: Factual, Analytical, Opinion, Contextual\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        model (str): 사용할 LLM 모델\n",
    "\n",
    "    Returns:\n",
    "        str: 분류된 질의 유형\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: LLM에게 분류 기준과 출력 형식을 안내\n",
    "    system_prompt = \"\"\"귀하는 질문을 분류하는 전문가입니다.\n",
    "    주어진 쿼리를 다음 카테고리 중 정확히 한 가지로 분류하세요:\n",
    "    - 사실: 구체적이고 검증 가능한 정보를 찾는 쿼리.\n",
    "    - 분석적: 종합적인 분석이나 설명이 필요한 쿼리.\n",
    "    - 의견: 주관적인 사안에 대한 질의 또는 다양한 관점을 추구하는 질의.\n",
    "    - 컨텍스트: 사용자별 컨텍스트에 따라 달라지는 쿼리.\n",
    "\n",
    "    설명이나 추가 텍스트 없이 카테고리 이름만 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 질의를 포함한 프롬프트 구성\n",
    "    user_prompt = f\"Classify this query: {query}\"\n",
    "    \n",
    "    # LLM을 호출하여 질의 분류 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 분류 결과 추출 및 정제\n",
    "    category = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 유효한 분류 카테고리 정의\n",
    "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "    \n",
    "    # 응답이 유효한 카테고리 중 하나에 포함되는지 확인\n",
    "    for valid in valid_categories:\n",
    "        if valid in category:\n",
    "            return valid\n",
    "    \n",
    "    # 분류 실패 시 기본값으로 \"Factual\" 반환\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Specialized Retrieval Strategies\n",
    "### 1. Factual Strategy - Focus on Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    사실 기반 질의에 적합한 검색 전략 (정확도 중심)\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서 목록\n",
    "    \"\"\"\n",
    "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # 질의 정밀도를 높이기 위한 LLM 기반 질의 개선 프롬프트 정의\n",
    "    system_prompt = \"\"\"귀하는 검색 쿼리를 개선하는 전문가입니다.\n",
    "    귀하의 임무는 주어진 사실 쿼리를 재구성하여 정보 검색을 위해 더 정확하고\n",
    "    정보 검색을 위해 구체화하는 것입니다. 주요 엔터티와 그 관계에 집중하세요.\n",
    "\n",
    "    설명 없이 개선된 쿼리만 제공하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Enhance this factual query: {query}\"\n",
    "    \n",
    "    # LLM을 통해 개선된 질의 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 개선된 질의 추출 및 출력\n",
    "    enhanced_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    \n",
    "    # 개선된 질의에 대해 임베딩 생성\n",
    "    query_embedding = create_embeddings(enhanced_query)\n",
    "    \n",
    "    # 유사도 검색을 통해 후보 문서 검색 (후보 수는 2배로 확장)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # 문서별 관련성 평가 결과 저장용 리스트 초기화\n",
    "    ranked_results = []\n",
    "    \n",
    "    # LLM을 통해 각 문서의 질의에 대한 관련성 점수 산정\n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    # 관련성 점수를 기준으로 결과 정렬 (내림차순)\n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # 상위 k개 문서 반환\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analytical Strategy - Comprehensive Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    분석형(Analytical) 질의에 대한 검색 전략: 주제 전반을 포괄하는 정보 중심\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서 목록\n",
    "    \"\"\"\n",
    "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # LLM이 복잡한 질의를 하위 질문으로 분해하도록 유도하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"귀하는 복잡한 질문을 세분화하는 데 전문가입니다.\n",
    "    기본 분석 쿼리의 다양한 측면을 탐구하는 하위 질문을 생성하세요.\n",
    "    이러한 하위 질문은 주제를 폭넓게 다루어야 하고\n",
    "    포괄적인 정보를 검색하는 데 도움이 되어야 합니다.\n",
    "\n",
    "    정확히 3개의 하위 질문 목록을 한 줄에 하나씩 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 질의를 포함한 프롬프트 생성\n",
    "    user_prompt = f\"이 분석 쿼리에 대한 하위 질문 생성하기: {query}\"\n",
    "    \n",
    "    # LLM을 통해 하위 질문(sub-questions) 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # 응답에서 하위 질문 추출 및 정리\n",
    "    sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
    "    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "    print(f\"Generated sub-queries: {sub_queries}\")\n",
    "    \n",
    "    # 각 하위 질문에 대해 문서 검색 수행\n",
    "    all_results = []\n",
    "    for sub_query in sub_queries:\n",
    "        sub_query_embedding = create_embeddings(sub_query)\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # 중복 문서 제거 및 다양한 출처 확보\n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result[\"text\"])\n",
    "            diverse_results.append(result)\n",
    "    \n",
    "    # 필요한 문서 수가 부족할 경우, 원래 질의로 직접 검색하여 보완\n",
    "    if len(diverse_results) < k:\n",
    "        main_query_embedding = create_embeddings(query)\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result in main_results:\n",
    "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result[\"text\"])\n",
    "                diverse_results.append(result)\n",
    "    \n",
    "    # 상위 k개의 결과만 반환\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opinion Strategy - Diverse Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    의견형 질의에 대한 검색 전략\n",
    "    다양한 관점을 중심으로 정보를 수집하여 응답의 균형성과 깊이를 높이는 목적\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서 목록\n",
    "    \"\"\"\n",
    "    print(f\"의견형 검색 전략 실행 중: '{query}'\")\n",
    "\n",
    "    # LLM이 다양한 관점을 생성하도록 유도하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"귀하는 한 주제에 대한 다양한 관점을 식별하는 데 전문가입니다.\n",
    "    의견이나 관점에 대한 주어진 쿼리에 대해 사람들이 이 주제에 대해 가질 수 있는 다양한 관점을 식별하세요.\n",
    "    정확히 3개의 서로 다른 관점을 한 줄에 하나씩 반환하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 질의를 포함한 프롬프트 구성\n",
    "    user_prompt = f\"Identify different perspectives on: {query}\"\n",
    "\n",
    "    # LLM을 통해 관점 목록 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    # 응답에서 관점 목록 추출 및 정제\n",
    "    viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
    "    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
    "    print(f\"도출된 관점 목록: {viewpoints}\")\n",
    "\n",
    "    # 각 관점에 대해 문서 검색 수행\n",
    "    all_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # 원래 질의에 관점을 결합하여 쿼리를 강화\n",
    "        combined_query = f\"{query} {viewpoint}\"\n",
    "        viewpoint_embedding = create_embeddings(combined_query)\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "\n",
    "        # 검색 결과에 해당 관점 정보 포함\n",
    "        for result in results:\n",
    "            result[\"viewpoint\"] = viewpoint\n",
    "\n",
    "        all_results.extend(results)\n",
    "\n",
    "    # 관점별로 하나씩 대표 문서를 선정\n",
    "    selected_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "\n",
    "    # 부족한 문서 수는 유사도 순으로 추가 확보\n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "\n",
    "    # 최종 k개의 문서 반환\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contextual Strategy - User Context Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    컨텍스트 기반 질의에 대한 검색 전략\n",
    "    사용자 맥락을 통합하여 검색 관련성을 향상시키는 방식\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서 수\n",
    "        user_context (str): 명시적 또는 질의로부터 추론된 사용자 맥락 정보\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서 목록\n",
    "    \"\"\"\n",
    "    print(f\"컨텍스트 기반 검색 전략 실행 중: '{query}'\")\n",
    "\n",
    "    # 사용자 맥락이 명시되지 않은 경우, LLM을 통해 질의로부터 추론\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"귀하는 질문에 내포된 문맥을 이해하는 데 전문가입니다.\n",
    "        주어진 쿼리에 대해 어떤 문맥 정보가 관련되거나 암시되는지 추론하세요.\n",
    "        이 쿼리에 답하는 데 도움이 될 수 있는 배경 정보를 중심으로 작성하세요.\n",
    "        간결한 문장 하나로 암시된 컨텍스트를 반환하세요.\"\"\"\n",
    "\n",
    "        user_prompt = f\"이 쿼리에서 암시된 컨텍스트 추론하기: {query}\"\n",
    "\n",
    "        # LLM에게 컨텍스트 추론 요청\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        # 추론된 사용자 맥락 추출\n",
    "        user_context = response.choices[0].message.content.strip()\n",
    "        print(f\"추론된 컨텍스트: {user_context}\")\n",
    "\n",
    "    # 질의에 사용자 맥락을 통합하여 질의를 재구성\n",
    "    system_prompt = \"\"\"귀하는 문맥에 맞게 질문을 재구성하는 데 전문가입니다.\n",
    "    쿼리와 주어진 컨텍스트 정보를 기반으로 보다 구체적이고 명확한 질의로 바꿔주세요.\n",
    "    설명 없이 재작성된 쿼리만 출력하세요.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {user_context}\n",
    "\n",
    "    이 컨텍스트를 반영하여 쿼리를 다시 작성하세요.\"\"\"\n",
    "\n",
    "    # LLM에게 질의 재작성 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 재작성된 컨텍스트 기반 질의 추출\n",
    "    contextualized_query = response.choices[0].message.content.strip()\n",
    "    print(f\"컨텍스트 반영 질의: {contextualized_query}\")\n",
    "\n",
    "    # 재작성된 질의에 대한 임베딩 생성 및 문서 검색 수행\n",
    "    query_embedding = create_embeddings(contextualized_query)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "\n",
    "    # 검색된 문서들에 대해 문맥 관련성 평가\n",
    "    ranked_results = []\n",
    "    for doc in initial_results:\n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "\n",
    "    # 맥락 관련성 기준으로 정렬 후 상위 k개 문서 반환\n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Document Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_document_relevance(query, document, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 문서의 질의에 대한 관련성을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        document (str): 평가할 문서 텍스트\n",
    "        model (str): 사용할 LLM 모델\n",
    "\n",
    "    Returns:\n",
    "        float: 0~10 사이의 관련성 점수\n",
    "    \"\"\"\n",
    "    # LLM에게 문서 관련성 평가 방법을 안내하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"귀하는 문서 관련성을 평가하는 전문가입니다.\n",
    "    문서와 쿼리의 관련성을 0에서 10까지의 척도로 평가하세요:\n",
    "    0 = 전혀 관련 없음\n",
    "    10 = 쿼리를 완벽하게 해결\n",
    "\n",
    "    0에서 10 사이의 숫자 점수만 반환하고 그 외에는 반환하지 않습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 문서가 너무 길 경우 미리보기로 잘라내기\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # 사용자 프롬프트 구성: 질의와 문서 제공 후 관련성 점수 요청\n",
    "    user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document: {doc_preview}\n",
    "\n",
    "        Relevance score (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # LLM 호출을 통해 관련성 점수 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 응답으로부터 점수 텍스트 추출\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 정규표현식을 이용하여 숫자 추출\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # 점수를 0~10 범위로 제한\n",
    "    else:\n",
    "        # 점수 추출 실패 시 기본값 반환\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_document_context_relevance(query, context, document, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    질의와 사용자 맥락을 함께 고려하여 문서의 관련성을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 사용자 맥락 정보\n",
    "        document (str): 평가할 문서 텍스트\n",
    "        model (str): 사용할 LLM 모델\n",
    "\n",
    "    Returns:\n",
    "        float: 0~10 사이의 관련성 점수\n",
    "    \"\"\"\n",
    "    # LLM에게 질의 + 맥락 기반으로 문서 관련성 평가 방법을 안내하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"귀하는 문맥을 고려하여 문서 관련성을 평가하는 전문가입니다.\n",
    "    문서가 쿼리를 얼마나 잘 처리하는지에 따라 0에서 10까지의 척도로 평가하세요.\n",
    "    쿼리를 얼마나 잘 처리하는지에 따라 0에서 10까지 평가합니다:\n",
    "    0 = 전혀 관련 없음\n",
    "    10 = 주어진 맥락에서 쿼리를 완벽하게 해결함\n",
    "\n",
    "    0에서 10 사이의 숫자 점수만 반환하고 다른 점수는 반환하지 않습니다.\n",
    "\"\"\"\n",
    "\n",
    "    # 문서가 너무 길 경우, 앞부분만 사용하여 평가하도록 자름\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # 사용자 프롬프트 구성: 질의, 맥락, 문서를 함께 제공하고 점수 요청\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    컨텍스트를 고려한 관련성 점수(0~10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # LLM을 호출하여 점수 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 응답으로부터 점수 텍스트 추출\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 정규표현식을 이용하여 숫자 추출\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # 점수를 0~10 범위로 제한\n",
    "    else:\n",
    "        # 점수 추출 실패 시 기본값 반환\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Adaptive Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    질의 유형에 따라 적절한 검색 전략을 자동으로 선택하여 실행하는 적응형 검색 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 검색할 문서 수\n",
    "        user_context (str): 사용자 맥락 정보 (컨텍스트 기반 질의에 사용)\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서 리스트\n",
    "    \"\"\"\n",
    "    # 질의 유형 분류 (Factual, Analytical, Opinion, Contextual 중 하나)\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"분류된 질의 유형: {query_type}\")\n",
    "\n",
    "    # 질의 유형에 따라 대응하는 검색 전략 실행\n",
    "    if query_type == \"Factual\":\n",
    "        # 사실 기반 정보 검색\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "        # 분석적 사고나 복합적 개념이 필요한 경우\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "        # 다양한 관점이나 의견을 요구하는 질의\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "        # 사용자의 맥락 정보가 중요한 질의\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "        # 분류되지 않은 경우, 기본적으로 사실 기반 검색 수행\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "\n",
    "    # 최종 검색 결과 반환\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(query, results, query_type, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    질의 유형과 검색된 문서를 기반으로 적절한 응답을 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        results (List[Dict]): 검색된 문서 목록\n",
    "        query_type (str): 질의 유형 (Factual, Analytical, Opinion, Contextual 등)\n",
    "        model (str): 사용할 LLM 모델\n",
    "\n",
    "    Returns:\n",
    "        str: 생성된 응답 텍스트\n",
    "    \"\"\"\n",
    "    # 검색된 문서 내용을 하나의 context로 구성\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "\n",
    "    # 질의 유형에 따라 시스템 프롬프트 설정\n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"귀하는 사실 기반 정보를 제공하는 조력자입니다.\n",
    "        제공된 문맥을 바탕으로 질문에 정확하고 간결하게 답하세요.\n",
    "        문맥에 정보가 부족하다면 그 한계를 명확히 밝혀주세요.\"\"\"\n",
    "\n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"귀하는 분석적 설명을 제공하는 조력자입니다.\n",
    "        문맥을 종합적으로 분석하고 주제의 여러 측면을 다루어 주세요.\n",
    "        정보 간 충돌이 있다면 이를 인정하고 균형 있게 설명하세요.\"\"\"\n",
    "\n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"귀하는 다양한 관점을 제시하는 조력자입니다.\n",
    "        주제에 대한 여러 시각을 공정하게 나열하고 편향되지 않게 설명하세요.\n",
    "        관점이 제한적일 경우 그 점도 함께 알려주세요.\"\"\"\n",
    "\n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"귀하는 문맥에 민감하게 반응하는 조력자입니다.\n",
    "        질문과 문맥을 모두 고려하여 연관성 높은 응답을 제공합니다.\n",
    "        문맥이 부족하거나 불완전한 경우 그 한계를 분명히 하세요.\"\"\"\n",
    "\n",
    "    else:\n",
    "        system_prompt = \"\"\"귀하는 유용한 조력자입니다. 문맥을 바탕으로 질문에 답하고, 문맥이 부족할 경우 명확하게 밝혀주세요.\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성\n",
    "    user_prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    위 문맥을 바탕으로 질문에 적절한 응답을 작성하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # LLM 호출을 통해 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    # 최종 응답 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Adaptive Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    적응형 검색을 활용한 RAG(Retrieval-Augmented Generation) 파이프라인 전체 실행 함수\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): 처리할 PDF 문서 경로\n",
    "        query (str): 사용자 질의\n",
    "        k (int): 검색할 문서 수\n",
    "        user_context (str): (선택 사항) 사용자 맥락 정보\n",
    "\n",
    "    Returns:\n",
    "        Dict: 질의, 질의 유형, 검색된 문서, 응답을 포함한 결과 딕셔너리\n",
    "    \"\"\"\n",
    "    print(\"\\n***RAG WITH ADAPTIVE RETRIEVAL***\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 1단계: PDF 문서에서 텍스트 추출, 청크 분할, 임베딩 생성\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 2단계: 질의 유형 분류 (Factual, Analytical, Opinion, Contextual)\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # 3단계: 분류된 유형에 따라 적절한 검색 전략 실행\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # 4단계: 질의, 검색 문서, 질의 유형을 기반으로 응답 생성\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "    \n",
    "    # 5단계: 결과 딕셔너리 구성\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n***RESPONSE***\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    적응형 검색과 표준 검색을 테스트 질의에 대해 비교 평가하는 함수\n",
    "\n",
    "    이 함수는 다음의 과정을 수행한다:\n",
    "    - 문서를 전처리하여 텍스트 청크 및 벡터 저장소 생성\n",
    "    - 각 질의에 대해 표준 검색과 적응형 검색을 모두 수행\n",
    "    - 참조 정답이 제공된 경우, 생성된 응답과 비교하여 품질을 평가\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): 지식 기반으로 사용할 PDF 문서 경로\n",
    "        test_queries (List[str]): 테스트용 질의 리스트\n",
    "        reference_answers (List[str], optional): 응답 평가를 위한 참조 정답 리스트\n",
    "\n",
    "    Returns:\n",
    "        Dict: 각 질의에 대한 검색 및 응답 결과, 평가 점수를 포함한 딕셔너리\n",
    "    \"\"\"\n",
    "    print(\"적응형 검색 vs. 표준 검색 성능 평가 시작\")\n",
    "\n",
    "    # 문서 처리: 텍스트 추출, 청크 분할, 벡터 저장소 구축\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "\n",
    "    # 결과 저장용 리스트 초기화\n",
    "    results = []\n",
    "\n",
    "    # 각 테스트 질의에 대해 검색 및 응답 생성 수행\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n질의 {i+1}: {query}\")\n",
    "\n",
    "        # 표준 검색 수행\n",
    "        print(\"\\n표준 검색 실행 중\")\n",
    "        query_embedding = create_embeddings(query)\n",
    "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "        standard_response = generate_response(query, standard_docs, \"General\")\n",
    "\n",
    "        # 적응형 검색 수행\n",
    "        print(\"\\n적응형 검색 실행 중\")\n",
    "        query_type = classify_query(query)\n",
    "        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
    "        adaptive_response = generate_response(query, adaptive_docs, query_type)\n",
    "\n",
    "        # 결과 저장\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": query_type,\n",
    "            \"standard_retrieval\": {\n",
    "                \"documents\": standard_docs,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"documents\": adaptive_docs,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 참조 정답이 존재할 경우 함께 저장\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            result[\"reference_answer\"] = reference_answers[i]\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        # 응답 미리보기 출력\n",
    "        print(\"\\n응답 비교\")\n",
    "        print(f\"표준 검색 응답: {standard_response[:200]}...\")\n",
    "        print(f\"적응형 검색 응답: {adaptive_response[:200]}...\")\n",
    "\n",
    "    # 참조 정답이 있는 경우 응답 품질 비교 평가 수행\n",
    "    if reference_answers:\n",
    "        comparison = compare_responses(results)\n",
    "        print(\"\\n응답 비교 평가 결과\")\n",
    "        print(comparison)\n",
    "    else:\n",
    "        comparison = \"참조 정답이 없어 평가 생략됨\"\n",
    "\n",
    "    # 최종 결과 반환\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_responses(results):\n",
    "    \"\"\"\n",
    "    표준 검색과 적응형 검색의 응답을 참조 정답과 비교하여 분석하는 함수\n",
    "\n",
    "    Args:\n",
    "        results (List[Dict]): 표준 및 적응형 응답이 포함된 결과 리스트\n",
    "\n",
    "    Returns:\n",
    "        str: 응답 비교 분석 결과 텍스트\n",
    "    \"\"\"\n",
    "    # AI 모델이 응답 비교를 수행할 수 있도록 시스템 프롬프트 정의\n",
    "    comparison_prompt = \"\"\"귀하는 정보 검색 시스템의 전문 평가자입니다.\n",
    "    각 쿼리에 대한 표준 검색 응답과 적응형 검색 응답을 비교하세요.\n",
    "    정확성, 관련성, 포괄성, 참조 답변과의 일치성 등을 기준으로 분석하세요.\n",
    "    각 방식의 강점과 약점에 대해 한글로 자세히 설명하세요.\"\"\"\n",
    "\n",
    "    # 분석 결과를 담을 텍스트 초기화\n",
    "    comparison_text = \"표준 검색 vs 적응형 검색 응답 평가\\n\\n\"\n",
    "\n",
    "    # 각 질의 결과에 대해 비교 수행\n",
    "    for i, result in enumerate(results):\n",
    "        # 참조 정답이 없는 경우는 평가 생략\n",
    "        if \"reference_answer\" not in result:\n",
    "            continue\n",
    "\n",
    "        # 질의 정보 기록\n",
    "        comparison_text += f\"질의 {i+1}: {result['query']}\\n\"\n",
    "        comparison_text += f\"(질의 유형: {result['query_type']})\\n\\n\"\n",
    "        comparison_text += f\"[참조 정답]\\n{result['reference_answer']}\\n\\n\"\n",
    "\n",
    "        # 표준 검색 응답 기록\n",
    "        comparison_text += f\"[표준 검색 응답]\\n{result['standard_retrieval']['response']}\\n\\n\"\n",
    "\n",
    "        # 적응형 검색 응답 기록\n",
    "        comparison_text += f\"[적응형 검색 응답]\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n",
    "\n",
    "        # 사용자 프롬프트 구성\n",
    "        user_prompt = f\"\"\"\n",
    "        Reference Answer: {result['reference_answer']}\n",
    "\n",
    "        Standard Retrieval Response: {result['standard_retrieval']['response']}\n",
    "\n",
    "        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n",
    "\n",
    "        두 응답을 비교 분석해주세요.\n",
    "        \"\"\"\n",
    "\n",
    "        # AI 모델을 사용해 비교 분석 생성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": comparison_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        # 분석 결과 추가\n",
    "        comparison_text += f\"[비교 분석 결과]\\n{response.choices[0].message.content}\\n\\n\"\n",
    "\n",
    "    # 전체 비교 분석 결과 반환\n",
    "    return comparison_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Adaptive Retrieval System (Customized Queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 지식 소스 문서의 경로 설정\n",
    "# 이 PDF 파일은 RAG 시스템이 활용할 정보를 포함하고 있음\n",
    "pdf_path = \"../../dataset/AI_Understanding.pdf\"\n",
    "\n",
    "# 다양한 질의 유형을 포함하는 테스트 질의 정의\n",
    "# 적응형 검색(adaptive retrieval)이 질의 의도를 어떻게 처리하는지 시연\n",
    "test_queries = [\n",
    "    \"설명 가능한 AI(XAI)란 무엇인가요?\", # 사실 기반 질의 - 정의/구체 정보 요청\n",
    "    # \"How do AI ethics and governance frameworks address potential societal impacts?\", # 분석형 질의 - 포괄적 분석 필요\n",
    "    # \"Is AI development moving too fast for proper regulation?\", # 의견형 질의 - 다양한 관점 요청\n",
    "    # \"How might explainable AI help in healthcare decisions?\", # 문맥형 질의 - 사용자 상황 고려 필요\n",
    "]\n",
    "\n",
    "# 보다 철저한 평가를 위한 참조 정답 정의\n",
    "# 응답 품질을 객관적으로 평가하는 데 사용 가능\n",
    "reference_answers = [\n",
    "    \"설명 가능한 AI(XAI)는 의사 결정 방식에 대한 명확한 설명을 제공함으로써 AI 시스템을 투명하고 이해하기 쉽게 만드는 것을 목표로 합니다. 이는 사용자가 AI 기술을 신뢰하고 효과적으로 관리하는 데 도움이 됩니다.\",\n",
    "    # \"AI 윤리 및 거버넌스 프레임워크는 AI 시스템이 책임감 있게 개발되고 사용될 수 있도록 가이드라인과 원칙을 수립하여 잠재적인 사회적 영향을 해결합니다. 이러한 프레임워크는 공정성, 책임성, 투명성, 인권 보호에 중점을 두어 위험을 완화하고 유익한 결과를 촉진합니다.\",\n",
    "    # \"AI 개발이 너무 빠르게 진행되어 적절한 규제가 필요한지에 대한 의견은 다양합니다. 일부에서는 빠른 발전이 규제 노력보다 빨라 잠재적인 위험과 윤리적 문제를 야기한다고 주장합니다. 다른 사람들은 새로운 도전 과제를 해결하기 위해 규제가 함께 진화하면서 혁신이 현재의 속도로 계속되어야 한다고 생각합니다.\",\n",
    "    # \"설명 가능한 AI는 AI 기반 추천에 대한 투명하고 이해하기 쉬운 인사이트를 제공함으로써 의료진의 의사결정에 큰 도움을 줄 수 있습니다. 이러한 투명성은 의료 전문가가 AI 시스템을 신뢰하고, 정보에 입각한 결정을 내리고, AI 제안의 근거를 이해함으로써 환자 치료 결과를 개선하는 데 도움이 됩니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "적응형 검색 vs. 표준 검색 성능 평가 시작\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트를 청크 단위로 분할 중...\n",
      "21개의 텍스트 청크 생성 완료\n",
      "청크에 대한 임베딩 생성 중...\n",
      "총 21개의 청크가 벡터 저장소에 추가됨\n",
      "\n",
      "질의 1: 설명 가능한 AI(XAI)란 무엇인가요?\n",
      "\n",
      "표준 검색 실행 중\n",
      "\n",
      "적응형 검색 실행 중\n",
      "분류된 질의 유형: Factual\n",
      "Executing Factual retrieval strategy for: '설명 가능한 AI(XAI)란 무엇인가요?'\n",
      "Enhanced query: 설명 가능한 인공지능(XAI)의 정의와 주요 특징은 무엇인가요?\n",
      "\n",
      "응답 비교\n",
      "표준 검색 응답: 설명 가능한 AI(XAI)는 AI 시스템을 더욱 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 기술입니다. XAI는 AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공하여 사용자가 AI의 신뢰성과 공정성을 평가할 수 있도록 돕습니다. 이를 통해 AI 시스템에 대한 신뢰와 책임감을 향상시키고, 의사 결정 과정의 투명성을 높이는 것이 중요합니다. X...\n",
      "적응형 검색 응답: 설명 가능한 AI(XAI)는 AI 시스템을 더욱 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 기술입니다. XAI는 AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공하여 신뢰와 책임감을 향상시키기 위해 개발되고 있습니다. 이를 통해 사용자는 AI의 결정 과정을 이해하고 평가할 수 있게 됩니다....\n",
      "\n",
      "응답 비교 평가 결과\n",
      "표준 검색 vs 적응형 검색 응답 평가\n",
      "\n",
      "질의 1: 설명 가능한 AI(XAI)란 무엇인가요?\n",
      "(질의 유형: Factual)\n",
      "\n",
      "[참조 정답]\n",
      "설명 가능한 AI(XAI)는 의사 결정 방식에 대한 명확한 설명을 제공함으로써 AI 시스템을 투명하고 이해하기 쉽게 만드는 것을 목표로 합니다. 이는 사용자가 AI 기술을 신뢰하고 효과적으로 관리하는 데 도움이 됩니다.\n",
      "\n",
      "[표준 검색 응답]\n",
      "설명 가능한 AI(XAI)는 AI 시스템을 더욱 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 기술입니다. XAI는 AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공하여 사용자가 AI의 신뢰성과 공정성을 평가할 수 있도록 돕습니다. 이를 통해 AI 시스템에 대한 신뢰와 책임감을 향상시키고, 의사 결정 과정의 투명성을 높이는 것이 중요합니다. XAI는 AI의 편향성과 불공정성을 해결하는 데에도 기여할 수 있습니다.\n",
      "\n",
      "[적응형 검색 응답]\n",
      "설명 가능한 AI(XAI)는 AI 시스템을 더욱 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 기술입니다. XAI는 AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공하여 신뢰와 책임감을 향상시키기 위해 개발되고 있습니다. 이를 통해 사용자는 AI의 결정 과정을 이해하고 평가할 수 있게 됩니다.\n",
      "\n",
      "[비교 분석 결과]\n",
      "### 표준 검색 응답(Standard Retrieval Response)\n",
      "\n",
      "**강점:**\n",
      "1. **정확성**: 표준 검색 응답은 설명 가능한 AI(XAI)의 정의와 목적을 명확하게 설명하고 있습니다. AI 시스템의 투명성과 이해 용이성을 강조하며, 사용자가 AI의 신뢰성과 공정성을 평가할 수 있도록 돕는다는 점을 잘 설명합니다.\n",
      "2. **포괄성**: XAI의 중요성과 역할에 대해 더 많은 세부 정보를 제공합니다. AI의 편향성과 불공정성을 해결하는 데 기여할 수 있다는 추가적인 정보는 독자가 XAI의 필요성을 더 깊이 이해하는 데 도움이 됩니다.\n",
      "3. **관련성**: 사용자가 AI 시스템을 신뢰하고 책임감을 느끼도록 돕는 측면을 강조하여, XAI의 실질적인 이점을 잘 설명합니다.\n",
      "\n",
      "**약점:**\n",
      "1. **복잡성**: 정보가 다소 복잡하게 구성되어 있어, 간단한 설명을 원하는 사용자에게는 다소 부담스러울 수 있습니다.\n",
      "2. **중복성**: 일부 내용이 반복적으로 언급되어 있어, 간결함이 떨어질 수 있습니다.\n",
      "\n",
      "### 적응형 검색 응답(Adaptive Retrieval Response)\n",
      "\n",
      "**강점:**\n",
      "1. **간결성**: 적응형 검색 응답은 정보가 간결하게 정리되어 있어, 빠르게 이해할 수 있습니다. 사용자가 필요한 정보를 쉽게 찾을 수 있도록 돕습니다.\n",
      "2. **명확성**: XAI의 목적과 기능을 간단명료하게 설명하여, 비전문가도 쉽게 이해할 수 있는 장점이 있습니다.\n",
      "\n",
      "**약점:**\n",
      "1. **정확성 부족**: 표준 검색 응답에 비해 XAI의 역할에 대한 설명이 다소 부족합니다. AI의 편향성과 불공정성을 해결하는 측면이 언급되지 않아, XAI의 중요성을 충분히 전달하지 못합니다.\n",
      "2. **포괄성 부족**: 정보가 간결한 대신, XAI의 다양한 측면이나 이점에 대한 설명이 부족하여, 독자가 XAI에 대한 깊이 있는 이해를 얻기 어렵습니다.\n",
      "\n",
      "### 결론\n",
      "표준 검색 응답은 XAI에 대한 포괄적이고 깊이 있는 정보를 제공하여 사용자가 AI 시스템의 신뢰성과 공정성을 평가하는 데 도움이 됩니다. 반면, 적응형 검색 응답은 간결하고 명확한 정보를 제공하지만, XAI의 다양한 측면을 충분히 설명하지 못해 정보의 깊이가 부족합니다. 따라서 사용자의 필요에 따라 두 응답 방식의 선택이 달라질 수 있으며, 정보의 복잡성과 깊이를 고려하여 적절한 응답 방식을 선택하는 것이 중요합니다.\n",
      "\n",
      "\n",
      "표준 검색 vs 적응형 검색 응답 평가\n",
      "\n",
      "질의 1: 설명 가능한 AI(XAI)란 무엇인가요?\n",
      "(질의 유형: Factual)\n",
      "\n",
      "[참조 정답]\n",
      "설명 가능한 AI(XAI)는 의사 결정 방식에 대한 명확한 설명을 제공함으로써 AI 시스템을 투명하고 이해하기 쉽게 만드는 것을 목표로 합니다. 이는 사용자가 AI 기술을 신뢰하고 효과적으로 관리하는 데 도움이 됩니다.\n",
      "\n",
      "[표준 검색 응답]\n",
      "설명 가능한 AI(XAI)는 AI 시스템을 더욱 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 기술입니다. XAI는 AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공하여 사용자가 AI의 신뢰성과 공정성을 평가할 수 있도록 돕습니다. 이를 통해 AI 시스템에 대한 신뢰와 책임감을 향상시키고, 의사 결정 과정의 투명성을 높이는 것이 중요합니다. XAI는 AI의 편향성과 불공정성을 해결하는 데에도 기여할 수 있습니다.\n",
      "\n",
      "[적응형 검색 응답]\n",
      "설명 가능한 AI(XAI)는 AI 시스템을 더욱 투명하고 이해하기 쉽게 만드는 것을 목표로 하는 기술입니다. XAI는 AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공하여 신뢰와 책임감을 향상시키기 위해 개발되고 있습니다. 이를 통해 사용자는 AI의 결정 과정을 이해하고 평가할 수 있게 됩니다.\n",
      "\n",
      "[비교 분석 결과]\n",
      "### 표준 검색 응답(Standard Retrieval Response)\n",
      "\n",
      "**강점:**\n",
      "1. **정확성**: 표준 검색 응답은 설명 가능한 AI(XAI)의 정의와 목적을 명확하게 설명하고 있습니다. AI 시스템의 투명성과 이해 용이성을 강조하며, 사용자가 AI의 신뢰성과 공정성을 평가할 수 있도록 돕는다는 점을 잘 설명합니다.\n",
      "2. **포괄성**: XAI의 중요성과 역할에 대해 더 많은 세부 정보를 제공합니다. AI의 편향성과 불공정성을 해결하는 데 기여할 수 있다는 추가적인 정보는 독자가 XAI의 필요성을 더 깊이 이해하는 데 도움이 됩니다.\n",
      "3. **관련성**: 사용자가 AI 시스템을 신뢰하고 책임감을 느끼도록 돕는 측면을 강조하여, XAI의 실질적인 이점을 잘 설명합니다.\n",
      "\n",
      "**약점:**\n",
      "1. **복잡성**: 정보가 다소 복잡하게 구성되어 있어, 간단한 설명을 원하는 사용자에게는 다소 부담스러울 수 있습니다.\n",
      "2. **중복성**: 일부 내용이 반복적으로 언급되어 있어, 간결함이 떨어질 수 있습니다.\n",
      "\n",
      "### 적응형 검색 응답(Adaptive Retrieval Response)\n",
      "\n",
      "**강점:**\n",
      "1. **간결성**: 적응형 검색 응답은 정보가 간결하게 정리되어 있어, 빠르게 이해할 수 있습니다. 사용자가 필요한 정보를 쉽게 찾을 수 있도록 돕습니다.\n",
      "2. **명확성**: XAI의 목적과 기능을 간단명료하게 설명하여, 비전문가도 쉽게 이해할 수 있는 장점이 있습니다.\n",
      "\n",
      "**약점:**\n",
      "1. **정확성 부족**: 표준 검색 응답에 비해 XAI의 역할에 대한 설명이 다소 부족합니다. AI의 편향성과 불공정성을 해결하는 측면이 언급되지 않아, XAI의 중요성을 충분히 전달하지 못합니다.\n",
      "2. **포괄성 부족**: 정보가 간결한 대신, XAI의 다양한 측면이나 이점에 대한 설명이 부족하여, 독자가 XAI에 대한 깊이 있는 이해를 얻기 어렵습니다.\n",
      "\n",
      "### 결론\n",
      "표준 검색 응답은 XAI에 대한 포괄적이고 깊이 있는 정보를 제공하여 사용자가 AI 시스템의 신뢰성과 공정성을 평가하는 데 도움이 됩니다. 반면, 적응형 검색 응답은 간결하고 명확한 정보를 제공하지만, XAI의 다양한 측면을 충분히 설명하지 못해 정보의 깊이가 부족합니다. 따라서 사용자의 필요에 따라 두 응답 방식의 선택이 달라질 수 있으며, 정보의 복잡성과 깊이를 고려하여 적절한 응답 방식을 선택하는 것이 중요합니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 적응형 검색과 표준 검색을 비교하는 평가 실행\n",
    "# 각 질의를 두 방법으로 처리하고 결과를 비교함\n",
    "evaluation_results = evaluate_adaptive_vs_standard(\n",
    "    pdf_path=pdf_path,                  # 지식 추출을 위한 소스 문서\n",
    "    test_queries=test_queries,          # 평가할 테스트 질의 목록\n",
    "    reference_answers=reference_answers  # 비교를 위한 참조 정답 (옵션)\n",
    ")\n",
    "\n",
    "# 결과는 표준 검색과 적응형 검색의 성능을 질의 유형별로 비교하여\n",
    "# 적응형 전략이 더 나은 결과를 제공하는 경우를 강조함\n",
    "print(evaluation_results[\"comparison\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
