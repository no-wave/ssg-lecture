{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Multi-Modal RAG with Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Modal RAG 구현 절차\n",
    "\n",
    "**1.  문서 수집 및 텍스트+이미지 추출:**\n",
    "PDF, HTML, PPT 등에서 텍스트와 함께 이미지 요소를 추출한다.\n",
    "이미지의 위치, 캡션, 섹션 정보 등 메타데이터도 함께 저장한다.\n",
    "\n",
    "**2. 이미지 캡션 생성 (Image Captioning):**\n",
    "추출된 이미지를 비전 모델(예: BLIP, OFA, GPT-4V 등)을 활용하여 자동으로 설명을 생성한다.\n",
    "예시: 입력 이미지: 막대 그래프\n",
    "생성 캡션: “2023년과 2024년의 월별 매출을 비교한 막대 차트 입니다.”\n",
    "\n",
    "**3. 텍스트와 이미지 설명 통합:**\n",
    "생성된 이미지 캡션과 원본 텍스트를 하나의 인덱싱 대상으로 구성한다.\n",
    "문단 또는 섹션 기준으로 텍스트 + 시각 정보 묶음을 생성한다.\n",
    "\n",
    "**4. 임베딩 및 저장:**\n",
    "텍스트와 이미지 설명을 텍스트 임베딩 모델로 벡터화한다.\n",
    "벡터 DB(예: FAISS, Qdrant 등)에 저장하여 질의 시 검색 가능하도록 구성한다.\n",
    "\n",
    "**5. 질의 처리 및 검색:**\n",
    "사용자 질문을 벡터화하여, 텍스트 및 이미지 설명을 포함한 세그먼트에서 유사도 기반 검색한다.\n",
    "시각적 정보 기반 질문도 대응 가능하다.\n",
    "\n",
    "**6. 멀티모달 응답 생성:**\n",
    "검색된 텍스트 + 이미지 설명 기반으로 LLM이 최종 응답 생성한다.\n",
    "필요 시, 원본 이미지 URL, 차트 위치 등을 함께 출력할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import base64\n",
    "import re\n",
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_content_from_pdf(pdf_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트와 이미지를 모두 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        output_dir (str, 선택): 이미지 저장 디렉토리 (없으면 임시 디렉토리 생성)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: 텍스트 정보 리스트, 이미지 정보 리스트\n",
    "    \"\"\"\n",
    "    # output_dir가 없으면 임시 디렉토리 생성\n",
    "    temp_dir = None\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        output_dir = temp_dir\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    text_data = []     # 추출된 텍스트 저장 리스트\n",
    "    image_paths = []   # 추출된 이미지 경로 및 메타데이터 저장 리스트\n",
    "\n",
    "    print(f\"{pdf_path}에서 콘텐츠 추출 중...\")\n",
    "\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # 각 페이지 순회\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "\n",
    "                # 텍스트 추출\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    text_data.append({\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "                # 이미지 추출\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]  # 이미지의 XREF\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "\n",
    "                    if base_image:\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "\n",
    "                        # 이미지 파일 저장\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "                        img_path = os.path.join(output_dir, img_filename)\n",
    "\n",
    "                        with open(img_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "\n",
    "                        image_paths.append({\n",
    "                            \"path\": img_path,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "\n",
    "        print(f\"텍스트 {len(text_data)}개, 이미지 {len(image_paths)}개 추출 완료\")\n",
    "        return text_data, image_paths\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"콘텐츠 추출 중 오류 발생: {e}\")\n",
    "        if temp_dir and os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Text Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 오버랩을 포함한 청크로 분할합니다.\n",
    "\n",
    "    Args:\n",
    "        text_data (List[Dict]): PDF에서 추출된 텍스트 데이터 (텍스트 + 메타데이터 포함)\n",
    "        chunk_size (int): 각 청크의 문자 수\n",
    "        overlap (int): 청크 간 중첩 문자 수\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 분할된 텍스트 청크 리스트 (각 청크에 메타데이터 포함)\n",
    "    \"\"\"\n",
    "    chunked_data = []  # 청크 데이터를 저장할 리스트 초기화\n",
    "\n",
    "    for item in text_data:\n",
    "        text = item[\"content\"]  # 텍스트 본문 추출\n",
    "        metadata = item[\"metadata\"]  # 해당 텍스트의 메타데이터 추출\n",
    "\n",
    "        # 텍스트가 너무 짧으면 그대로 저장 (절반 이하일 경우)\n",
    "        if len(text) < chunk_size / 2:\n",
    "            chunked_data.append({\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 지정된 크기와 오버랩에 따라 청크 생성\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]  # 청크 추출\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        # 생성된 각 청크에 메타데이터 추가\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = metadata.copy()  # 기존 메타데이터 복사\n",
    "            chunk_metadata[\"chunk_index\"] = i  # 청크 인덱스\n",
    "            chunk_metadata[\"chunk_count\"] = len(chunks)  # 전체 청크 수\n",
    "\n",
    "            chunked_data.append({\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": chunk_metadata\n",
    "            })\n",
    "\n",
    "    print(f\"총 {len(chunked_data)}개의 텍스트 청크가 생성되었습니다.\")\n",
    "    return chunked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning with OpenAI Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    이미지 파일을 base64 문자열로 인코딩합니다.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): 이미지 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        str: base64로 인코딩된 이미지 문자열\n",
    "    \"\"\"\n",
    "    # 이미지 파일을 바이너리 읽기 모드로 열기\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # 파일 내용을 읽고 base64로 인코딩\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        # base64 바이트 데이터를 문자열로 디코딩하여 반환\n",
    "        return encoded_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def generate_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    OpenAI의 비전 기능을 사용하여 이미지 캡션을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 이미지 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 이미지 설명 캡션\n",
    "    \"\"\"\n",
    "    # 파일이 존재하는지 및 이미지인지 확인\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"오류: 이미지 파일을 찾을 수 없습니다.\"\n",
    "    \n",
    "    try:\n",
    "        # 이미지를 열어 유효성 검사\n",
    "        Image.open(image_path)\n",
    "        \n",
    "        # 이미지를 base64로 인코딩\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        # 캡션 생성을 위한 API 요청 구성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  \n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"당신은 학술 논문 이미지 설명에 특화된 AI 어시스턴트입니다. \"\n",
    "                    \"이미지의 핵심 정보를 포착하는 상세한 캡션을 작성하세요. \"\n",
    "                    \"이미지에 차트, 표, 도해 등이 포함되어 있다면, 해당 내용과 목적을 명확하게 설명하세요. \"\n",
    "                    \"생성된 캡션은 이후 사용자가 이 콘텐츠에 대해 질문할 때 검색 가능하도록 최적화되어야 합니다.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"이 이미지를 학술적 관점에서 자세히 설명해 주세요:\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # 응답에서 캡션 추출\n",
    "        caption = response.choices[0].message.content\n",
    "        return caption\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 예외 발생 시 오류 메시지 반환\n",
    "        return f\"캡션 생성 중 오류 발생: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_images(image_paths):\n",
    "    \"\"\"\n",
    "    모든 이미지를 처리하여 캡션을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        image_paths (List[Dict]): 추출된 이미지들의 경로 및 메타데이터 리스트\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 캡션이 포함된 이미지 정보 리스트\n",
    "    \"\"\"\n",
    "    image_data = []  # 캡션 포함 이미지 데이터를 저장할 리스트 초기화\n",
    "\n",
    "    print(f\"{len(image_paths)}개의 이미지에 대해 캡션 생성 중...\")  # 총 이미지 수 출력\n",
    "    for i, img_item in enumerate(image_paths):\n",
    "        print(f\"{i+1}/{len(image_paths)} 번째 이미지 처리 중...\")  # 현재 이미지 진행상황 출력\n",
    "        img_path = img_item[\"path\"]  # 이미지 경로 추출\n",
    "        metadata = img_item[\"metadata\"]  # 이미지 메타데이터 추출\n",
    "\n",
    "        # 이미지에 대한 캡션 생성\n",
    "        caption = generate_image_caption(img_path)\n",
    "\n",
    "        # 캡션이 포함된 이미지 데이터 추가\n",
    "        image_data.append({\n",
    "            \"content\": caption,        # 생성된 캡션\n",
    "            \"metadata\": metadata,      # 이미지 메타데이터\n",
    "            \"image_path\": img_path     # 이미지 파일 경로\n",
    "        })\n",
    "\n",
    "    return image_data  # 최종 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    멀티모달 콘텐츠를 위한 간단한 벡터 저장소 구현입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 벡터, 콘텐츠, 메타데이터를 저장할 리스트 초기화\n",
    "        self.vectors = []\n",
    "        self.contents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        단일 항목을 벡터 저장소에 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            content (str): 텍스트 또는 이미지 캡션 등 콘텐츠\n",
    "            embedding (List[float]): 임베딩 벡터\n",
    "            metadata (Dict, 선택): 추가 메타데이터\n",
    "        \"\"\"\n",
    "        # 임베딩, 콘텐츠, 메타데이터를 각각의 리스트에 추가\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        여러 항목을 벡터 저장소에 일괄 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): 콘텐츠와 메타데이터를 포함한 항목 리스트\n",
    "            embeddings (List[List[float]]): 각 항목에 대한 임베딩 리스트\n",
    "        \"\"\"\n",
    "        # 각 항목과 임베딩을 순회하며 저장소에 추가\n",
    "        for item, embedding in zip(items, embeddings):\n",
    "            self.add_item(\n",
    "                content=item[\"content\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        쿼리 임베딩과 가장 유사한 항목들을 검색합니다.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): 쿼리 임베딩 벡터\n",
    "            k (int): 반환할 유사 항목 수\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개의 유사 항목\n",
    "        \"\"\"\n",
    "        # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 임베딩을 넘파이 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 계산하여 유사도 리스트 구성\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도 기준으로 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 추출 및 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # JSON 직렬화를 위한 float 변환\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 리스트에 대해 임베딩 벡터를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): 입력 텍스트 리스트\n",
    "        model (str): 사용할 임베딩 모델 이름\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 생성된 임베딩 벡터 리스트\n",
    "    \"\"\"\n",
    "    # 입력이 비어 있는 경우 빈 리스트 반환\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # OpenAI 또는 HuggingFace API 제한을 고려하여 배치 단위 처리\n",
    "    batch_size = 100\n",
    "    all_embeddings = []  # 전체 임베딩을 저장할 리스트\n",
    "    \n",
    "    # 입력 텍스트를 배치 단위로 순회하며 처리\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # 현재 배치 추출\n",
    "        \n",
    "        # 현재 배치에 대해 임베딩 생성 요청\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # 응답에서 임베딩 벡터 추출\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # 전체 임베딩 리스트에 추가\n",
    "    \n",
    "    # 모든 임베딩 반환\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    멀티모달 RAG 처리를 위한 문서를 처리합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 텍스트 청크의 길이 (문자 수 기준)\n",
    "        chunk_overlap (int): 청크 간 중첩 길이 (문자 수 기준)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[MultiModalVectorStore, Dict]: 벡터 저장소와 문서 처리 요약 정보\n",
    "    \"\"\"\n",
    "    # 이미지 추출 저장 경로 생성\n",
    "    image_dir = \"extracted_images\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    \n",
    "    # PDF에서 텍스트와 이미지 추출\n",
    "    text_data, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n",
    "    \n",
    "    # 추출된 텍스트를 청크 단위로 분할\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # 이미지에 대해 캡션 생성\n",
    "    image_data = process_images(image_paths)\n",
    "    \n",
    "    # 텍스트 청크와 이미지 설명을 하나로 합침\n",
    "    all_items = chunked_text + image_data\n",
    "    \n",
    "    # 임베딩 생성을 위한 텍스트만 추출\n",
    "    contents = [item[\"content\"] for item in all_items]\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    print(\"전체 콘텐츠에 대해 임베딩 생성 중...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # 벡터 저장소 생성 및 항목 추가\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(all_items, embeddings)\n",
    "    \n",
    "    # 문서 정보 요약 (텍스트, 이미지 개수 등)\n",
    "    doc_info = {\n",
    "        \"text_count\": len(chunked_text),\n",
    "        \"image_count\": len(image_data),\n",
    "        \"total_items\": len(all_items),\n",
    "    }\n",
    "    \n",
    "    # 요약 출력\n",
    "    print(f\"벡터 저장소에 총 {len(all_items)}개 항목 추가 완료 \"\n",
    "          f\"({len(chunked_text)}개 텍스트 청크, {len(image_data)}개 이미지 캡션)\")\n",
    "    \n",
    "    # 벡터 저장소와 문서 요약 정보 반환\n",
    "    return vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_multimodal_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    멀티모달 RAG 시스템에 쿼리를 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store (MultiModalVectorStore): 문서 콘텐츠가 저장된 벡터 저장소\n",
    "        k (int): 검색할 관련 항목 개수\n",
    "\n",
    "    Returns:\n",
    "        Dict: 쿼리 결과 및 생성된 응답\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 쿼리 처리 중: {query} ===\\n\")\n",
    "    \n",
    "    # 쿼리에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # 벡터 저장소에서 관련 콘텐츠 검색\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # 검색 결과를 텍스트와 이미지로 구분\n",
    "    text_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"text\"]\n",
    "    image_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"image\"]\n",
    "    \n",
    "    print(f\"관련 항목 {len(results)}개 검색됨 (텍스트 {len(text_results)}개, 이미지 캡션 {len(image_results)}개)\")\n",
    "    \n",
    "    # 검색된 콘텐츠를 기반으로 AI 응답 생성\n",
    "    response = generate_response(query, results)\n",
    "    \n",
    "    # 결과 딕셔너리로 정리하여 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"response\": response,\n",
    "        \"text_results_count\": len(text_results),\n",
    "        \"image_results_count\": len(image_results)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(query, results):\n",
    "    \"\"\"\n",
    "    쿼리와 검색된 결과를 기반으로 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        results (List[Dict]): 검색된 텍스트 및 이미지 콘텐츠\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답 텍스트\n",
    "    \"\"\"\n",
    "    # 검색된 결과들을 기반으로 문맥(context) 구성\n",
    "    context = \"\"\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        # 콘텐츠 유형 판단 (텍스트 또는 이미지 캡션)\n",
    "        content_type = \"Text\" if result[\"metadata\"].get(\"type\") == \"text\" else \"Image caption\"\n",
    "        # 페이지 번호 추출 (없으면 unknown)\n",
    "        page_num = result[\"metadata\"].get(\"page\", \"unknown\")\n",
    "        \n",
    "        # 콘텐츠 유형 및 페이지 정보 추가\n",
    "        context += f\"[{content_type} from page {page_num}]\\n\"\n",
    "        # 실제 콘텐츠 추가\n",
    "        context += result[\"content\"]\n",
    "        context += \"\\n\\n\"\n",
    "    \n",
    "    # 시스템 프롬프트 (AI에게 역할 및 응답 방식 지시)\n",
    "    system_message = \"\"\"당신은 텍스트와 이미지를 포함한 문서를 기반으로 질문에 답변하는 AI 어시스턴트입니다. \n",
    "    당신은 문서로부터 검색된 텍스트와 이미지 캡션을 제공받았습니다. \n",
    "    이 정보를 사용하여 질문에 대해 정확하고 포괄적인 답변을 작성하십시오.\n",
    "    만약 이미지나 도표에서 얻은 정보라면, 그 출처를 언급하십시오.\n",
    "    검색된 정보만으로 질문에 완전히 답변할 수 없다면, 그 한계를 인정하십시오.\"\"\"\n",
    "\n",
    "    # 사용자 메시지: 질문 + 문맥\n",
    "    user_message = f\"\"\"질문: {query}\n",
    "\n",
    "    검색된 콘텐츠:\n",
    "    {context}\n",
    "\n",
    "    위 내용을 기반으로 질문에 답변해 주세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # LLM을 호출하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.1  # 정확도 중심으로 창의성 최소화\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 텍스트 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Against Text-Only RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    비교용 텍스트 전용 벡터 저장소를 구축합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 청크의 문자 수\n",
    "        chunk_overlap (int): 청크 간 중첩 문자 수\n",
    "\n",
    "    Returns:\n",
    "        MultiModalVectorStore: 텍스트 전용 벡터 저장소\n",
    "    \"\"\"\n",
    "    # PDF에서 텍스트만 추출 (이미지 무시)\n",
    "    text_data, _ = extract_content_from_pdf(pdf_path, None)\n",
    "    \n",
    "    # 텍스트를 청크 단위로 분할\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # 임베딩 생성을 위한 텍스트만 추출\n",
    "    contents = [item[\"content\"] for item in chunked_text]\n",
    "    \n",
    "    # 텍스트 임베딩 생성\n",
    "    print(\"텍스트 전용 콘텐츠에 대해 임베딩 생성 중...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # 벡터 저장소 생성 및 항목 추가\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(chunked_text, embeddings)\n",
    "    \n",
    "    print(f\"텍스트 전용 벡터 저장소에 {len(chunked_text)}개 항목 추가 완료\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_multimodal_vs_textonly(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    멀티모달 RAG와 텍스트 전용 RAG를 비교 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 평가할 PDF 파일 경로\n",
    "        test_queries (List[str]): 테스트 질문 리스트\n",
    "        reference_answers (List[str], 선택): 기준 정답 리스트\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 각 쿼리에 대한 평가 결과 및 전체 분석\n",
    "    \"\"\"\n",
    "    print(\"=== 멀티모달 RAG vs 텍스트 전용 RAG 평가 시작 ===\\n\")\n",
    "    \n",
    "    # 멀티모달 RAG용 문서 처리 (텍스트 + 이미지)\n",
    "    print(\"\\n멀티모달 RAG용 문서 처리 중...\")\n",
    "    mm_vector_store, mm_doc_info = process_document(pdf_path)\n",
    "    \n",
    "    # 텍스트 전용 RAG용 문서 처리\n",
    "    print(\"\\n텍스트 전용 RAG용 문서 처리 중...\")\n",
    "    text_vector_store = build_text_only_store(pdf_path)\n",
    "    \n",
    "    results = []  # 평가 결과 저장 리스트\n",
    "    \n",
    "    # 각 쿼리에 대해 평가 실행\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== 쿼리 {i+1} 평가 중: {query} ===\")\n",
    "        \n",
    "        # 기준 정답이 있다면 참조\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # 멀티모달 RAG 응답 생성\n",
    "        print(\"\\n멀티모달 RAG 실행 중...\")\n",
    "        mm_result = query_multimodal_rag(query, mm_vector_store)\n",
    "        \n",
    "        # 텍스트 전용 RAG 응답 생성\n",
    "        print(\"\\n텍스트 전용 RAG 실행 중...\")\n",
    "        text_result = query_multimodal_rag(query, text_vector_store)\n",
    "        \n",
    "        # 응답 비교 평가\n",
    "        comparison = compare_responses(\n",
    "            query,\n",
    "            mm_result[\"response\"],\n",
    "            text_result[\"response\"],\n",
    "            reference\n",
    "        )\n",
    "        \n",
    "        # 현재 쿼리 결과 저장\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"multimodal_response\": mm_result[\"response\"],\n",
    "            \"textonly_response\": text_result[\"response\"],\n",
    "            \"multimodal_results\": {\n",
    "                \"text_count\": mm_result[\"text_results_count\"],\n",
    "                \"image_count\": mm_result[\"image_results_count\"]\n",
    "            },\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison\n",
    "        })\n",
    "    \n",
    "    # 전체 평가 분석 생성\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,                     # 각 쿼리별 평가 결과\n",
    "        \"overall_analysis\": overall_analysis,   # 총괄 분석\n",
    "        \"multimodal_doc_info\": mm_doc_info      # 멀티모달 처리 문서 정보\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_responses(query, mm_response, text_response, reference=None):\n",
    "    \"\"\"\n",
    "    멀티모달 응답과 텍스트 전용 응답을 비교 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        mm_response (str): 멀티모달 RAG의 응답\n",
    "        text_response (str): 텍스트 전용 RAG의 응답\n",
    "        reference (str, 선택): 기준 정답 (있으면 정확성 비교 가능)\n",
    "\n",
    "    Returns:\n",
    "        str: 응답 비교 분석 결과\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 평가자 역할 및 비교 기준 설명\n",
    "    system_prompt = \"\"\"당신은 두 가지 RAG 시스템을 비교 평가하는 전문가입니다:\n",
    "    1. 멀티모달 RAG: 텍스트와 이미지 캡션 모두에서 검색\n",
    "    2. 텍스트 전용 RAG: 텍스트에서만 검색\n",
    "\n",
    "    다음 기준에 따라 어느 응답이 질문에 더 잘 답했는지 평가하세요:\n",
    "    - 정확성과 올바름\n",
    "    - 정보의 완전성\n",
    "    - 쿼리와의 관련성\n",
    "    - 이미지 요소에서 온 고유한 정보 (멀티모달의 경우)\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트: 쿼리와 두 응답을 포함\n",
    "    user_prompt = f\"\"\"질문: {query}\n",
    "\n",
    "    멀티모달 RAG 응답:\n",
    "    {mm_response}\n",
    "\n",
    "    텍스트 전용 RAG 응답:\n",
    "    {text_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # 기준 정답이 있는 경우 포함\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "    기준 정답:\n",
    "    {reference}\n",
    "    \"\"\"\n",
    "\n",
    "        user_prompt += \"\"\"\n",
    "    두 응답을 비교하고 어떤 응답이 질문에 더 잘 답변했는지, 그 이유를 설명하세요.\n",
    "    멀티모달 응답이에 이미지에서 온 구체적인 정보가 있다면 명시하십시오.\n",
    "    \"\"\"\n",
    "\n",
    "    # 평가 생성 (정확성 중시: temperature=0)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    멀티모달 RAG vs 텍스트 전용 RAG의 종합 분석을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): 각 쿼리에 대한 평가 결과 리스트\n",
    "\n",
    "    Returns:\n",
    "        str: 종합 분석 결과 (텍스트)\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 평가 기준 및 분석 방향 안내\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템을 평가하는 전문가입니다. 여러 개의 테스트 쿼리를 기반으로 \n",
    "    멀티모달 RAG(텍스트 + 이미지)와 텍스트 전용 RAG을 비교하여 종합 분석을 작성하세요.\n",
    "\n",
    "    다음 항목에 중점을 두십시오:\n",
    "    1. 멀티모달 RAG가 텍스트 전용보다 뛰어났던 쿼리 유형\n",
    "    2. 이미지 정보를 포함했을 때의 구체적 이점\n",
    "    3. 멀티모달 접근 방식의 단점 또는 제약 사항\n",
    "    4. 각 접근 방식을 언제 사용하는 것이 적절한지에 대한 전반적인 권장사항\"\"\"\n",
    "\n",
    "    # 쿼리별 요약 텍스트 생성\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"쿼리 {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"멀티모달은 텍스트 {result['multimodal_results']['text_count']}개, 이미지 캡션 {result['multimodal_results']['image_count']}개를 검색함\\n\"\n",
    "        evaluations_summary += f\"응답 비교 요약: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # 사용자 프롬프트: 평가 요약과 함께 전반적인 분석 요청\n",
    "    user_prompt = f\"\"\"다음은 총 {len(results)}개의 쿼리에 대해 멀티모달 RAG과 텍스트 전용 RAG의 평가 요약입니다. \n",
    "    이 데이터를 기반으로 두 접근 방식의 성능을 비교 분석해 주세요:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    이미지 정보가 응답 품질에 어떻게 기여했는지(또는 기여하지 못했는지)에 특히 주목하여 \n",
    "    멀티모달 RAG의 상대적인 강점과 약점을 포함한 포괄적인 분석을 작성해 주세요.\"\"\"\n",
    "\n",
    "    # LLM을 이용하여 분석 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0  # 분석의 일관성과 정확성을 위해 창의성 최소화\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Multi-Modal RAG vs Text-Only RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 멀티모달 RAG vs 텍스트 전용 RAG 평가 시작 ===\n",
      "\n",
      "\n",
      "멀티모달 RAG용 문서 처리 중...\n",
      "dataset/attention_is_all_you_need.pdf에서 콘텐츠 추출 중...\n",
      "텍스트 15개, 이미지 3개 추출 완료\n",
      "총 56개의 텍스트 청크가 생성되었습니다.\n",
      "3개의 이미지에 대해 캡션 생성 중...\n",
      "1/3 번째 이미지 처리 중...\n",
      "2/3 번째 이미지 처리 중...\n",
      "3/3 번째 이미지 처리 중...\n",
      "전체 콘텐츠에 대해 임베딩 생성 중...\n",
      "벡터 저장소에 총 59개 항목 추가 완료 (56개 텍스트 청크, 3개 이미지 캡션)\n",
      "\n",
      "텍스트 전용 RAG용 문서 처리 중...\n",
      "dataset/attention_is_all_you_need.pdf에서 콘텐츠 추출 중...\n",
      "텍스트 15개, 이미지 3개 추출 완료\n",
      "총 56개의 텍스트 청크가 생성되었습니다.\n",
      "텍스트 전용 콘텐츠에 대해 임베딩 생성 중...\n",
      "텍스트 전용 벡터 저장소에 56개 항목 추가 완료\n",
      "\n",
      "\n",
      "=== 쿼리 1 평가 중: Transformer (base model)의 BLEU 점수는 얼마인가요? ===\n",
      "\n",
      "멀티모달 RAG 실행 중...\n",
      "\n",
      "=== 쿼리 처리 중: Transformer (base model)의 BLEU 점수는 얼마인가요? ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3597273/4126536131.py:75: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # JSON 직렬화를 위한 float 변환\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관련 항목 5개 검색됨 (텍스트 5개, 이미지 캡션 0개)\n",
      "\n",
      "텍스트 전용 RAG 실행 중...\n",
      "\n",
      "=== 쿼리 처리 중: Transformer (base model)의 BLEU 점수는 얼마인가요? ===\n",
      "\n",
      "관련 항목 5개 검색됨 (텍스트 5개, 이미지 캡션 0개)\n",
      "\n",
      "***전체 분석 결과***\n",
      "\n",
      "### 멀티모달 RAG와 텍스트 전용 RAG 비교 분석\n",
      "\n",
      "#### 1. 멀티모달 RAG가 텍스트 전용보다 뛰어났던 쿼리 유형\n",
      "이번 쿼리에서는 \"Transformer (base model)의 BLEU 점수는 얼마인가요?\"라는 질문에 대해 멀티모달 RAG가 텍스트 5개를 검색했지만 이미지 캡션은 포함되지 않았습니다. 이 경우, 멀티모달 RAG의 성능이 텍스트 전용 RAG와 동등하게 나타났습니다. 그러나 일반적으로 멀티모달 RAG는 다음과 같은 쿼리 유형에서 더 뛰어난 성능을 보일 수 있습니다:\n",
      "- **비주얼 정보가 중요한 쿼리**: 예를 들어, 특정 이미지의 내용을 설명하거나, 이미지와 관련된 데이터를 요구하는 경우.\n",
      "- **복합적인 정보 요구**: 텍스트와 이미지가 함께 제공되어야 하는 경우, 예를 들어, 제품 리뷰에서 이미지와 텍스트가 함께 제공될 때.\n",
      "\n",
      "#### 2. 이미지 정보를 포함했을 때의 구체적 이점\n",
      "이번 쿼리에서는 이미지 정보가 포함되지 않았지만, 일반적으로 멀티모달 RAG의 이미지 정보가 응답 품질에 기여하는 방식은 다음과 같습니다:\n",
      "- **시각적 맥락 제공**: 이미지가 텍스트의 의미를 보강하거나 명확히 할 수 있습니다. 예를 들어, 특정 제품의 이미지를 포함하면 해당 제품의 특성을 더 잘 이해할 수 있습니다.\n",
      "- **정보의 다양성**: 이미지와 텍스트가 결합되어 다양한 형태의 정보를 제공함으로써 사용자가 더 풍부한 정보를 얻을 수 있습니다.\n",
      "- **감정적 반응 유도**: 이미지가 포함되면 사용자의 감정적 반응을 유도할 수 있어, 정보의 수용성을 높일 수 있습니다.\n",
      "\n",
      "#### 3. 멀티모달 접근 방식의 단점 또는 제약 사항\n",
      "멀티모달 RAG의 단점은 다음과 같습니다:\n",
      "- **복잡성 증가**: 멀티모달 시스템은 텍스트와 이미지 모두를 처리해야 하므로, 시스템의 복잡성이 증가하고, 이는 처리 시간과 자원 소모를 증가시킬 수 있습니다.\n",
      "- **데이터의 일관성 문제**: 이미지와 텍스트 간의 일관성을 유지하는 것이 어려울 수 있으며, 잘못된 이미지가 제공될 경우 정보의 신뢰성이 떨어질 수 있습니다.\n",
      "- **제한된 이미지 데이터**: 특정 도메인에서는 이미지 데이터가 부족할 수 있으며, 이 경우 멀티모달 RAG의 이점이 감소합니다.\n",
      "\n",
      "#### 4. 각 접근 방식을 언제 사용하는 것이 적절한지에 대한 전반적인 권장사항\n",
      "- **텍스트 전용 RAG**: \n",
      "  - 텍스트 기반 정보가 주로 요구되는 경우, 예를 들어, 문서 요약, 텍스트 분석, 또는 특정 데이터 포인트에 대한 질문 등.\n",
      "  - 이미지 정보가 필요하지 않거나, 텍스트만으로 충분히 답변할 수 있는 경우.\n",
      "\n",
      "- **멀티모달 RAG**: \n",
      "  - 이미지와 텍스트가 모두 필요한 경우, 예를 들어, 제품 리뷰, 시각적 데이터 분석, 또는 이미지 설명이 필요한 쿼리 등.\n",
      "  - 사용자가 시각적 정보를 통해 더 나은 이해를 원할 때, 예를 들어, 교육 자료나 마케팅 콘텐츠 등.\n",
      "\n",
      "결론적으로, 멀티모달 RAG는 특정 상황에서 강력한 도구가 될 수 있지만, 모든 쿼리에 적합한 것은 아니며, 사용자의 요구와 쿼리의 특성에 따라 적절한 접근 방식을 선택하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# PDF 문서 경로 설정\n",
    "pdf_path = \"../../dataset/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# 텍스트 및 시각적 정보를 함께 타겟팅하는 테스트 쿼리 정의\n",
    "test_queries = [\n",
    "    \"Transformer (base model)의 BLEU 점수는 얼마인가요?\",\n",
    "]\n",
    "\n",
    "# (선택) 평가를 위한 기준 정답 제공\n",
    "reference_answers = [\n",
    "    \"Transformer (base model)은 WMT 2014 영어→독일어 번역 과제에서 BLEU 점수 27.3, 영어→프랑스어 과제에서 38.1을 달성합니다.\",\n",
    "]\n",
    "\n",
    "# 멀티모달 RAG vs 텍스트 전용 RAG 평가 실행\n",
    "evaluation_results = evaluate_multimodal_vs_textonly(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# 전체 분석 결과 출력\n",
    "print(\"\\n***전체 분석 결과***\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
