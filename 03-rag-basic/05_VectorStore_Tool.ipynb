{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHt4M2WQnwJs"
   },
   "source": [
    "# VectorStore\n",
    "\n",
    "1. ChromaDB\n",
    "2. FAISS\n",
    "3. Qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oU9k-7kanwJv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecacy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oU9k-7kanwJv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- OpenAI 클라이언트 설정 ---\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# --- 헬퍼 함수 (기존 노트북과 동일) ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"PDF 파일에서 텍스트를 추출합니다.\"\"\"\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"\n",
    "    for page in mypdf:\n",
    "        all_text += page.get_text(\"text\")\n",
    "    return all_text\n",
    "\n",
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"주어진 텍스트를 n자 단위로, 지정된 수의 문자가 겹치도록 분할합니다.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])\n",
    "    return chunks\n",
    "\n",
    "def generate_questions(text_chunk, num_questions=5, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"주어진 텍스트 청크로부터 관련 질문들을 생성합니다.\"\"\"\n",
    "    system_prompt = (\n",
    "        \"당신은 텍스트로부터 관련 질문을 생성하는 전문가입니다. \"\n",
    "        \"제공된 텍스트를 바탕으로 그 내용에만 근거한 간결한 질문들을 생성하세요.\"\n",
    "    )\n",
    "    user_prompt = f\"다음 텍스트를 기반으로, 해당 텍스트만으로 답할 수 있는 서로 다른 질문 {num_questions}개를 생성하세요:\\n\\n{text_chunk}\\n\\n응답은 번호가 매겨진 질문 리스트 형식으로만 작성하고, 그 외 부가 설명은 하지 마세요.\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    questions_text = response.choices[0].message.content.strip()\n",
    "    questions = [re.sub(r'^\\d+\\.\\s*', '', line.strip()) for line in questions_text.split('\\n') if line.strip()]\n",
    "    return [q for q in questions if q.endswith('?')]\n",
    "\n",
    "\n",
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"지정된 모델을 사용하여 입력 텍스트에 대한 임베딩을 생성합니다.\"\"\"\n",
    "    response = client.embeddings.create(model=model, input=text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl3Wm_5HnwJx"
   },
   "source": [
    "## 1. Chroma (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "grpcio-tools 1.73.1 requires protobuf<7.0.0,>=6.30.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "grpcio-health-checking 1.73.1 requires protobuf<7.0.0,>=6.30.0, but you have protobuf 5.29.5 which is incompatible.\n",
      "grpcio-status 1.73.0 requires protobuf<7.0.0,>=6.30.0, but you have protobuf 5.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UNAXJegSnwJx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 21개의 텍스트 청크 생성 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chroma 처리 중: 100%|██████████| 21/21 [00:54<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma에 총 84개의 항목 저장 완료.\n",
      "\n",
      "--- Chroma 검색 결과 ---\n",
      "Rank 1 (Distance: 0.6437):\n",
      "  Type: question\n",
      "  Text: 자율 무기 시스템에 AI를 사용할 때의 윤리적 우려는 무엇인가요?...\n",
      "--------------------\n",
      "Rank 2 (Distance: 0.7951):\n",
      "  Type: chunk\n",
      "  Text:  등에 \n",
      "사용됩니다. AI 알고리즘은 대규모 데이터 세트를 분석하여 패턴을 파악하고, 시장 동향을 \n",
      "예측하고, 금융 프로세스를 자동화할 수 있습니다. \n",
      "교통편 \n",
      "AI는 자율주행차, 교통 최적화 시스템, 물류 관리의 발전으로 교통 분야에 혁신을 일으키고 \n",
      "있습니다. 자율 주행 차량은 AI를 사용하여 주변 환경을 인식하고, 주행 결정을 내리고, \n",
      "안전하게 주행...\n",
      "--------------------\n",
      "Rank 3 (Distance: 0.8244):\n",
      "  Type: chunk\n",
      "  Text:  심각한 윤리적, 보안적 우려가 제기될 수 있습니다. AI \n",
      "기반 무기와 관련된 위험을 해결하기 위해 국제적인 논의와 규제가 필요합니다. \n",
      " \n",
      "5장: 인공 지능의 미래 \n",
      "AI의 미래는 다양한 영역에서 지속적인 발전과 폭넓은 도입으로 특징지어질 것입니다. \n",
      "주요 트렌드와 개발 분야는 다음과 같습니다: \n",
      "설명 가능한 AI(XAI) \n",
      "설명 가능한 AI(XAI)는...\n",
      "--------------------\n",
      "Rank 4 (Distance: 0.8851):\n",
      "  Type: chunk\n",
      "  Text: 동화하고 위협 탐지 정확도를 \n",
      "개선하며 전반적인 사이버 보안 태세를 강화할 수 있습니다. \n",
      " \n",
      "4장: AI의 윤리적, 사회적 의미 \n",
      "AI의 급속한 발전과 보급은 윤리적, 사회적으로 심각한 우려를 불러일으킵니다. 이러한 \n",
      "우려에는 다음이 포함됩니다: \n",
      "편견과 공정성 \n",
      "AI 시스템은 데이터에 존재하는 편견을 계승하고 증폭시켜 불공정하거나 차별적인 결과를 \n",
      "초래...\n",
      "--------------------\n",
      "Rank 5 (Distance: 0.8887):\n",
      "  Type: chunk\n",
      "  Text: 인공 지능 이해 \n",
      " \n",
      "1장: 인공 지능 소개 \n",
      "인공 지능(AI)은 디지털 컴퓨터 또는 컴퓨터로 제어되는 로봇이 지적인 존재와 일반적으로 \n",
      "관련된 작업을 수행할 수 있는 능력을 말합니다. 이 용어는 추론, 의미 발견, 일반화, 과거 \n",
      "경험으로부터의 학습 능력 등 인간의 특징적인 지적 프로세스가 부여된 시스템을 \n",
      "개발하는 프로젝트에 자주 적용됩니다. 지난 수십...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "def process_document_with_chroma(\n",
    "    pdf_path, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    questions_per_chunk=3\n",
    "):\n",
    "    \"\"\"문서를 처리하여 Chroma 컬렉션에 저장합니다.\"\"\"\n",
    "    # 1. 텍스트 추출 및 청크 분할\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"총 {len(text_chunks)}개의 텍스트 청크 생성 완료.\")\n",
    "\n",
    "    # 2. Chroma 클라이언트 및 컬렉션 설정\n",
    "    chroma_client = chromadb.Client() # 인-메모리 클라이언트\n",
    "    collection = chroma_client.get_or_create_collection(name=\"document_store_chroma\")\n",
    "\n",
    "    # 3. 각 청크 처리 및 Chroma에 추가\n",
    "    all_embeddings = []\n",
    "    all_documents = []\n",
    "    all_metadatas = []\n",
    "    all_ids = []\n",
    "\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Chroma 처리 중\")):\n",
    "        # 청크 임베딩\n",
    "        chunk_embedding = create_embeddings(chunk).data[0].embedding\n",
    "        all_embeddings.append(chunk_embedding)\n",
    "        all_documents.append(chunk)\n",
    "        all_metadatas.append({\"type\": \"chunk\", \"index\": i})\n",
    "        all_ids.append(f\"chunk_{i}\")\n",
    "\n",
    "        # 질문 생성 및 임베딩\n",
    "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
    "        if questions:\n",
    "            question_embeddings = create_embeddings(questions).data\n",
    "            for j, (q, q_emb) in enumerate(zip(questions, question_embeddings)):\n",
    "                all_embeddings.append(q_emb.embedding)\n",
    "                all_documents.append(q)\n",
    "                all_metadatas.append({\"type\": \"question\", \"chunk_index\": i, \"original_chunk\": chunk})\n",
    "                all_ids.append(f\"question_{i}_{j}\")\n",
    "\n",
    "    # 모든 데이터를 한 번에 추가\n",
    "    collection.add(\n",
    "        embeddings=all_embeddings,\n",
    "        documents=all_documents,\n",
    "        metadatas=all_metadatas,\n",
    "        ids=all_ids\n",
    "    )\n",
    "\n",
    "    print(f\"Chroma에 총 {collection.count()}개의 항목 저장 완료.\")\n",
    "    return collection\n",
    "\n",
    "# --- 실행 및 검색 예시 ---\n",
    "pdf_path = \"../dataset/AI_Understanding.pdf\"\n",
    "chroma_collection = process_document_with_chroma(pdf_path)\n",
    "\n",
    "# 검색 쿼리\n",
    "query_text = \"인공지능의 윤리적 문제는 무엇인가?\"\n",
    "query_embedding = create_embeddings(query_text).data[0].embedding\n",
    "\n",
    "# Chroma에서 유사도 검색 수행\n",
    "results = chroma_collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(\"\\n--- Chroma 검색 결과 ---\")\n",
    "for i, (doc, meta, dist) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):\n",
    "    print(f\"Rank {i+1} (Distance: {dist:.4f}):\")\n",
    "    print(f\"  Type: {meta.get('type')}\")\n",
    "    print(f\"  Text: {doc[:200]}...\") # 텍스트가 길 경우 일부만 출력\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6zxxULbnwJx"
   },
   "source": [
    "## 2. FAISS (Facebook AI Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q faiss-cpu \n",
    "# 또는 GPU 버전: faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yC9Wgu_KnwJx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 21개의 텍스트 청크 생성 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS 처리 중: 100%|██████████| 21/21 [00:50<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS에 총 84개의 항목 저장 완료.\n",
      "\n",
      "--- FAISS 검색 결과 ---\n",
      "Rank 1 (Similarity: 0.6725):\n",
      "  Type: question\n",
      "  Text: 인공 지능(AI)의 정의는 무엇인가요?...\n",
      "--------------------\n",
      "Rank 2 (Similarity: 0.6595):\n",
      "  Type: question\n",
      "  Text: 자율 무기 시스템에 AI를 사용할 때 발생할 수 있는 윤리적 우려는 무엇인가요?...\n",
      "--------------------\n",
      "Rank 3 (Similarity: 0.6506):\n",
      "  Type: question\n",
      "  Text: AI의 윤리적 우려 중 개인정보 보호와 데이터 보안이 중요한 이유는 무엇인가요?...\n",
      "--------------------\n",
      "Rank 4 (Similarity: 0.6500):\n",
      "  Type: question\n",
      "  Text: 윤리적 AI의 주요 원칙에는 어떤 것들이 포함되나요?...\n",
      "--------------------\n",
      "Rank 5 (Similarity: 0.6024):\n",
      "  Type: chunk\n",
      "  Text:  등에 \n",
      "사용됩니다. AI 알고리즘은 대규모 데이터 세트를 분석하여 패턴을 파악하고, 시장 동향을 \n",
      "예측하고, 금융 프로세스를 자동화할 수 있습니다. \n",
      "교통편 \n",
      "AI는 자율주행차, 교통 최적화 시스템, 물류 관리의 발전으로 교통 분야에 혁신을 일으키고 \n",
      "있습니다. 자율 주행 차량은 AI를 사용하여 주변 환경을 인식하고, 주행 결정을 내리고, \n",
      "안전하게 주행...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "def process_document_with_faiss(\n",
    "    pdf_path, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    questions_per_chunk=3\n",
    "):\n",
    "    \"\"\"문서를 처리하여 FAISS 인덱스와 메타데이터 리스트를 생성합니다.\"\"\"\n",
    "    # 1. 텍스트 추출 및 청크 분할\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"총 {len(text_chunks)}개의 텍스트 청크 생성 완료.\")\n",
    "\n",
    "    # 2. FAISS 인덱스 및 메타데이터 저장소 초기화\n",
    "    embedding_dim = 1536  # OpenAI 'text-embedding-3-small' 모델의 차원\n",
    "    index = faiss.IndexFlatIP(embedding_dim) # 코사인 유사도를 위한 내적(IP) 인덱스\n",
    "\n",
    "    # FAISS는 벡터 외 정보를 저장하지 않으므로 별도 리스트 관리\n",
    "    metadata_store = []\n",
    "\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"FAISS 처리 중\")):\n",
    "        # 청크 임베딩 및 정규화\n",
    "        chunk_embedding = create_embeddings(chunk).data[0].embedding\n",
    "        chunk_vector = np.array(chunk_embedding, dtype='float32')\n",
    "        faiss.normalize_L2(chunk_vector.reshape(1, -1))\n",
    "\n",
    "        # 인덱스와 메타데이터 저장소에 추가\n",
    "        index.add(chunk_vector.reshape(1, -1))\n",
    "        metadata_store.append({\"text\": chunk, \"type\": \"chunk\", \"index\": i})\n",
    "\n",
    "        # 질문 생성 및 임베딩\n",
    "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
    "        if questions:\n",
    "            question_embeddings_data = create_embeddings(questions).data\n",
    "            q_vectors = np.array([item.embedding for item in question_embeddings_data], dtype='float32')\n",
    "            faiss.normalize_L2(q_vectors)\n",
    "\n",
    "            index.add(q_vectors)\n",
    "            for j, q in enumerate(questions):\n",
    "                metadata_store.append({\"text\": q, \"type\": \"question\", \"chunk_index\": i, \"original_chunk\": chunk})\n",
    "\n",
    "    print(f\"FAISS에 총 {index.ntotal}개의 항목 저장 완료.\")\n",
    "    return index, metadata_store\n",
    "\n",
    "# --- 실행 및 검색 예시 ---\n",
    "pdf_path = \"../dataset/AI_Understanding.pdf\"\n",
    "faiss_index, faiss_metadata = process_document_with_faiss(pdf_path)\n",
    "\n",
    "# 검색 쿼리\n",
    "query_text = \"인공지능의 윤리적 문제는 무엇인가?\"\n",
    "query_embedding = create_embeddings(query_text).data[0].embedding\n",
    "query_vector = np.array(query_embedding, dtype='float32').reshape(1, -1)\n",
    "faiss.normalize_L2(query_vector) # 쿼리 벡터도 정규화\n",
    "\n",
    "# FAISS에서 유사도 검색 수행\n",
    "k = 5\n",
    "distances, indices = faiss_index.search(query_vector, k)\n",
    "\n",
    "print(\"\\n--- FAISS 검색 결과 ---\")\n",
    "for i in range(k):\n",
    "    idx = indices[0][i]\n",
    "    dist = distances[0][i]\n",
    "    meta = faiss_metadata[idx]\n",
    "\n",
    "    print(f\"Rank {i+1} (Similarity: {dist:.4f}):\")\n",
    "    print(f\"  Type: {meta.get('type')}\")\n",
    "    print(f\"  Text: {meta.get('text')[:200]}...\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wii1yMJ8nwJy"
   },
   "source": [
    "## 3. Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DitO4cLdnwJy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3155103/1847974152.py:20: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 21개의 텍스트 청크 생성 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qdrant 처리 중: 100%|██████████| 21/21 [00:48<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant에 총 84개의 항목 저장 완료.\n",
      "\n",
      "--- Qdrant 검색 결과 ---\n",
      "Rank 1 (Score: 0.6675):\n",
      "  Type: question\n",
      "  Text: 윤리적 AI 원칙의 주요 내용은 무엇입니까?...\n",
      "--------------------\n",
      "Rank 2 (Score: 0.6352):\n",
      "  Type: question\n",
      "  Text: 인공 지능의 정의는 무엇인가요?...\n",
      "--------------------\n",
      "Rank 3 (Score: 0.6025):\n",
      "  Type: chunk\n",
      "  Text:  등에 \n",
      "사용됩니다. AI 알고리즘은 대규모 데이터 세트를 분석하여 패턴을 파악하고, 시장 동향을 \n",
      "예측하고, 금융 프로세스를 자동화할 수 있습니다. \n",
      "교통편 \n",
      "AI는 자율주행차, 교통 최적화 시스템, 물류 관리의 발전으로 교통 분야에 혁신을 일으키고 \n",
      "있습니다. 자율 주행 차량은 AI를 사용하여 주변 환경을 인식하고, 주행 결정을 내리고, \n",
      "안전하게 주행...\n",
      "--------------------\n",
      "Rank 4 (Score: 0.5887):\n",
      "  Type: chunk\n",
      "  Text:  심각한 윤리적, 보안적 우려가 제기될 수 있습니다. AI \n",
      "기반 무기와 관련된 위험을 해결하기 위해 국제적인 논의와 규제가 필요합니다. \n",
      " \n",
      "5장: 인공 지능의 미래 \n",
      "AI의 미래는 다양한 영역에서 지속적인 발전과 폭넓은 도입으로 특징지어질 것입니다. \n",
      "주요 트렌드와 개발 분야는 다음과 같습니다: \n",
      "설명 가능한 AI(XAI) \n",
      "설명 가능한 AI(XAI)는...\n",
      "--------------------\n",
      "Rank 5 (Score: 0.5730):\n",
      "  Type: question\n",
      "  Text: AI 기반 무기와 관련된 윤리적, 보안적 우려를 해결하기 위해 필요한 조치는 무엇인가요?...\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3155103/1847974152.py:76: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "import uuid\n",
    "\n",
    "def process_document_with_qdrant(\n",
    "    pdf_path, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    questions_per_chunk=3\n",
    "):\n",
    "    \"\"\"문서를 처리하여 Qdrant 컬렉션에 저장합니다.\"\"\"\n",
    "    # 1. 텍스트 추출 및 청크 분할\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"총 {len(text_chunks)}개의 텍스트 청크 생성 완료.\")\n",
    "\n",
    "    # 2. Qdrant 클라이언트 및 컬렉션 설정\n",
    "    client = QdrantClient(\":memory:\") # 인-메모리 클라이언트\n",
    "    collection_name = \"document_store_qdrant\"\n",
    "    \n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=1536,  # OpenAI 'text-embedding-3-small' 모델의 차원\n",
    "            distance=models.Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 3. 각 청크 처리 및 Qdrant에 추가 (배치 처리)\n",
    "    points_to_upsert = []\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Qdrant 처리 중\")):\n",
    "        # 청크 임베딩\n",
    "        chunk_embedding = create_embeddings(chunk).data[0].embedding\n",
    "        \n",
    "        # Qdrant에 저장할 Point 생성\n",
    "        points_to_upsert.append(\n",
    "            models.PointStruct(\n",
    "                id=str(uuid.uuid4()), # 고유 ID 생성\n",
    "                vector=chunk_embedding,\n",
    "                payload={\"text\": chunk, \"type\": \"chunk\", \"index\": i}\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 질문 생성 및 임베딩\n",
    "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
    "        if questions:\n",
    "            question_embeddings_data = create_embeddings(questions).data\n",
    "            for q, q_emb in zip(questions, question_embeddings_data):\n",
    "                points_to_upsert.append(\n",
    "                    models.PointStruct(\n",
    "                        id=str(uuid.uuid4()),\n",
    "                        vector=q_emb.embedding,\n",
    "                        payload={\"text\": q, \"type\": \"question\", \"chunk_index\": i, \"original_chunk\": chunk}\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # 모든 Point를 한 번에 업로드 (Upsert)\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points_to_upsert,\n",
    "        wait=True # 작업 완료 대기\n",
    "    )\n",
    "    \n",
    "    count_result = client.count(collection_name=collection_name, exact=True)\n",
    "    print(f\"Qdrant에 총 {count_result.count}개의 항목 저장 완료.\")\n",
    "    return client, collection_name\n",
    "\n",
    "# --- 실행 및 검색 예시 ---\n",
    "pdf_path = \"../dataset/AI_Understanding.pdf\"\n",
    "qdrant_client, collection_name = process_document_with_qdrant(pdf_path)\n",
    "\n",
    "# 검색 쿼리\n",
    "query_text = \"인공지능의 윤리적 문제는 무엇인가?\"\n",
    "query_embedding = create_embeddings(query_text).data[0].embedding\n",
    "\n",
    "# Qdrant에서 유사도 검색 수행\n",
    "search_result = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_embedding,\n",
    "    limit=5 # 상위 5개 결과 반환\n",
    ")\n",
    "\n",
    "print(\"\\n--- Qdrant 검색 결과 ---\")\n",
    "for i, hit in enumerate(search_result):\n",
    "    print(f\"Rank {i+1} (Score: {hit.score:.4f}):\")\n",
    "    print(f\"  Type: {hit.payload.get('type')}\")\n",
    "    print(f\"  Text: {hit.payload.get('text')[:200]}...\")\n",
    "    print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
