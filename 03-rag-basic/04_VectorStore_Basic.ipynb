{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        str: 추출된 전체 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일을 엽니다.\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 텍스트를 저장할 문자열 초기화\n",
    "\n",
    "    # 각 페이지를 순회하면서 텍스트를 추출합니다.\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 해당 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 누적\n",
    "\n",
    "    return all_text  # 전체 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로, 지정된 수의 문자가 겹치도록 분할합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 분할할 원본 텍스트.\n",
    "        n (int): 각 청크의 문자 수.\n",
    "        overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 분할된 텍스트 청크 리스트.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap)만큼 이동하며 텍스트를 분할\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])  # 현재 위치부터 n자까지 슬라이싱하여 추가\n",
    "\n",
    "    return chunks  # 청크 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Questions for Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text_chunk, num_questions=5, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 청크로부터 관련 질문들을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text_chunk (str): 질문을 생성할 대상 텍스트 청크.\n",
    "        num_questions (int): 생성할 질문의 개수.\n",
    "        model (str): 사용할 언어 모델.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 생성된 질문 리스트.\n",
    "    \"\"\"\n",
    "    # AI의 역할을 정의하는 시스템 프롬프트\n",
    "    system_prompt = (\n",
    "        \"당신은 텍스트로부터 관련 질문을 생성하는 전문가입니다. \"\n",
    "        \"제공된 텍스트를 바탕으로 그 내용에만 근거한 간결한 질문들을 생성하세요. \"\n",
    "        \"핵심 정보와 개념에 초점을 맞추세요.\"\n",
    "    )\n",
    "    \n",
    "    # 사용자 프롬프트: 텍스트와 함께 질문 생성 요청\n",
    "    user_prompt = f\"\"\"\n",
    "    다음 텍스트를 기반으로, 해당 텍스트만으로 답할 수 있는 서로 다른 질문 {num_questions}개를 생성하세요:\n",
    "\n",
    "    {text_chunk}\n",
    "    \n",
    "    응답은 번호가 매겨진 질문 리스트 형식으로만 작성하고, 그 외 부가 설명은 하지 마세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델 호출을 통해 질문 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 응답에서 질문 문자열 추출\n",
    "    questions_text = response.choices[0].message.content.strip()\n",
    "    questions = []\n",
    "\n",
    "    # 줄 단위로 질문을 추출하고 정리\n",
    "    for line in questions_text.split('\\n'):\n",
    "        # 번호 제거 및 양쪽 공백 제거\n",
    "        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "        if cleaned_line and cleaned_line.endswith('?'):\n",
    "            questions.append(cleaned_line)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    지정된 모델을 사용하여 입력 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 임베딩을 생성할 입력 텍스트 또는 텍스트 리스트.\n",
    "        model (str): 사용할 임베딩 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        dict: 생성된 임베딩 정보를 포함한 OpenAI API의 응답 객체.\n",
    "    \"\"\"\n",
    "    # 입력 텍스트에 대해 임베딩 생성 요청\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    # 응답 객체 반환\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현 클래스입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \"\"\"\n",
    "        self.vectors = []    # 임베딩 벡터들을 저장\n",
    "        self.texts = []      # 원본 텍스트들을 저장\n",
    "        self.metadata = []   # 텍스트에 대한 메타데이터 저장\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트.\n",
    "            embedding (List[float]): 임베딩 벡터.\n",
    "            metadata (dict, optional): 추가 메타데이터 (기본값: None).\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))             # 벡터 추가\n",
    "        self.texts.append(text)                              # 텍스트 추가\n",
    "        self.metadata.append(metadata or {})                 # 메타데이터 추가\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        쿼리 임베딩과 가장 유사한 항목을 검색합니다.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 쿼리 벡터.\n",
    "            k (int): 반환할 결과 수 (기본값: 5).\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개의 유사 항목. 텍스트, 메타데이터, 유사도 포함.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "\n",
    "        # 각 벡터와의 코사인 유사도 계산\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Documents with Question Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200, questions_per_chunk=5):\n",
    "    \"\"\"\n",
    "    문서를 처리하고, 각 청크에 대해 질문을 생성하여 벡터 저장소에 추가합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로.\n",
    "        chunk_size (int): 각 청크의 문자 수.\n",
    "        chunk_overlap (int): 청크 간 중첩 문자 수.\n",
    "        questions_per_chunk (int): 청크당 생성할 질문 수.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore]: 생성된 텍스트 청크 리스트와 벡터 저장소 객체.\n",
    "    \"\"\"\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"텍스트 청크 분할 중...\")\n",
    "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"총 {len(text_chunks)}개의 텍스트 청크가 생성되었습니다.\")\n",
    "    \n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    print(\"각 청크에 대해 임베딩 및 질문 생성 중...\")\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"청크 처리 중\")):\n",
    "        # 청크 임베딩 생성\n",
    "        chunk_embedding_response = create_embeddings(chunk)\n",
    "        chunk_embedding = chunk_embedding_response.data[0].embedding\n",
    "        \n",
    "        # 청크를 벡터 저장소에 추가\n",
    "        vector_store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=chunk_embedding,\n",
    "            metadata={\"type\": \"chunk\", \"index\": i}\n",
    "        )\n",
    "        \n",
    "        # 해당 청크 기반 질문 생성\n",
    "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
    "        \n",
    "        # 각 질문에 대한 임베딩 생성 후 저장소에 추가\n",
    "        for j, question in enumerate(questions):\n",
    "            question_embedding_response = create_embeddings(question)\n",
    "            question_embedding = question_embedding_response.data[0].embedding\n",
    "            \n",
    "            vector_store.add_item(\n",
    "                text=question,\n",
    "                embedding=question_embedding,\n",
    "                metadata={\n",
    "                    \"type\": \"question\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"original_chunk\": chunk\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    return text_chunks, vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Processing the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF에서 텍스트 추출 중...\n",
      "텍스트 청크 분할 중...\n",
      "총 21개의 텍스트 청크가 생성되었습니다.\n",
      "각 청크에 대해 임베딩 및 질문 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "청크 처리 중: 100%|██████████| 21/21 [01:22<00:00,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소에 저장된 항목 수: 84개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 경로 정의\n",
    "pdf_path = \"../dataset/AI_Understanding.pdf\"\n",
    "\n",
    "# 문서 처리: 텍스트 추출, 청크 분할, 질문 생성, 벡터 저장소 구축\n",
    "text_chunks, vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000,       # 각 청크는 1000자\n",
    "    chunk_overlap=200,     # 청크 간 200자 중첩\n",
    "    questions_per_chunk=3  # 청크당 질문 3개 생성\n",
    ")\n",
    "\n",
    "# 벡터 저장소에 저장된 항목 개수 출력\n",
    "print(f\"벡터 저장소에 저장된 항목 수: {len(vector_store.texts)}개\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
