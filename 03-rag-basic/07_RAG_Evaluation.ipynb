{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtM8AX-gS_zh"
   },
   "source": [
    "# RAG Evaluation\n",
    "#### 작성: 고우주"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtM8AX-gS_zh"
   },
   "source": [
    "RAG(Retrieval-Augmented Generation) 모델의 성능을 평가하는 주요 지표들을 계산하는 파이썬 코드 예시입니다. 실제 평가에서는 더 정교한 자연어 이해(NLU) 기술이나 대규모 언어 모델(LLM)을 평가자로 활용하는 방안도 고려할 수 있지만, 여기서는 일반적으로 사용되는 라이브러리를 활용하여 각 지표의 개념을 이해하고 직접 산출해볼 수 있는 코드를 제공합니다.\n",
    "\n",
    "### 평가 시나리오 설정\n",
    "\n",
    "먼저, 평가를 위한 가상의 데이터를 정의합니다.\n",
    "\n",
    "  * **사용자 질문 (Question):** 사용자가 시스템에 입력한 질문\n",
    "  * **원본 텍스트 (Retrieved Context):** RAG 모델이 검색 단계에서 가져온 원본 문서 또는 문맥\n",
    "  * **생성된 답변 (Generated Answer):** RAG 모델이 검색된 문맥을 기반으로 최종 생성한 답변\n",
    "  * **정답 (Ground Truth Answer):** 사람이 직접 작성한 이상적인 답변"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 가상 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tFYq5evWS_zk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 사용자의 질문\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# RAG 모델이 검색한 문맥 (Retrieved Context)\n",
    "# 이 중 첫 번째, 두 번째 문장이 답변 생성에 직접적으로 기여했다고 가정\n",
    "retrieved_context = \"\"\"\n",
    "대한민국은 동아시아에 위치한 국가이다.\n",
    "수도는 서울특별시이며, 대한민국의 정치, 경제, 사회, 문화의 중심지 역할을 한다.\n",
    "서울의 인구는 약 940만 명이다.\n",
    "부산은 대한민국 제2의 도시이자 최대 항구 도시이다.\n",
    "\"\"\"\n",
    "\n",
    "# RAG 모델이 생성한 답변 (Generated Answer)\n",
    "generated_answer = \"대한민국의 수도는 서울입니다.\"\n",
    "\n",
    "# 정답 (Ground Truth)\n",
    "ground_truth_answer = \"대한민국의 수도는 서울특별시입니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install -q sentence-transformers scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리를 임포트\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 평가 시나리오 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 사용자 질문: 대한민국의 수도는 어디인가요?\n",
      "# 검색된 문맥: 대한민국은 동아시아에 위치한 국가이다.\n",
      "수도는 서울특별시이며, 대한민국의 정치, 경제, 사회, 문화의 중심지 역할을 한다.\n",
      "서울의 인구는 약 940만 명이다.\n",
      "부산은 대한민국 제2의 도시이자 최대 항구 도시이다.\n",
      "# 생성된 답변: 대한민국의 수도는 서울입니다.\n",
      "# 정답: 대한민국의 수도는 서울특별시입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자의 질문\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# RAG 모델이 검색한 문맥 (Retrieved Context)\n",
    "# 가정: 이 중 첫 번째, 두 번째 문장이 답변 생성에 직접적으로 기여\n",
    "retrieved_context = \"\"\"\n",
    "대한민국은 동아시아에 위치한 국가이다.\n",
    "수도는 서울특별시이며, 대한민국의 정치, 경제, 사회, 문화의 중심지 역할을 한다.\n",
    "서울의 인구는 약 940만 명이다.\n",
    "부산은 대한민국 제2의 도시이자 최대 항구 도시이다.\n",
    "\"\"\"\n",
    "# 문맥을 문장 단위로 분리합\n",
    "context_sentences = [s.strip() for s in retrieved_context.strip().split('\\n')]\n",
    "\n",
    "# RAG 모델이 생성한 답변\n",
    "generated_answer = \"대한민국의 수도는 서울입니다.\"\n",
    "\n",
    "# 정답 (Ground Truth)\n",
    "ground_truth_answer = \"대한민국의 수도는 서울특별시입니다.\"\n",
    "\n",
    "# 답변 생성에 실제로 필요한 이상적인 문맥(가정)\n",
    "# Context Recall 계산에 사용됩니다.\n",
    "ideal_context = \"대한민국의 수도는 서울특별시이다.\"\n",
    "\n",
    "print(f\"# 사용자 질문: {question}\")\n",
    "print(f\"# 검색된 문맥: {retrieved_context.strip()}\")\n",
    "print(f\"# 생성된 답변: {generated_answer}\")\n",
    "print(f\"# 정답: {ground_truth_answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 로드\n",
    "- 한국어 문장을 임베딩하기 위해 사전 훈련된 모델을 로드합니다.\n",
    "- 'ko-sroberta-multitask'는 한국어 문장의 의미를 잘 포착하는 모델 중 하나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "\n",
    "def get_word_tokens(text):\n",
    "    \"\"\"\n",
    "    간단한 토크나이저: 텍스트에서 단어(한글, 영어, 숫자)를 추출합니다.\n",
    "    \"\"\"\n",
    "    return set(re.findall(r'[\\w\\d]+', text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Faithfulness (충실성)\n",
    "- 생성된 답변이 검색된 문맥에 얼마나 충실한지를 평가합니다.\n",
    "- 생성된 답변과 문맥 간의 의미적 유사도를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness: 0.6679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_embedding = model.encode(generated_answer, convert_to_tensor=True)\n",
    "context_embedding = model.encode(retrieved_context, convert_to_tensor=True)\n",
    "faithfulness_score = util.pytorch_cos_sim(answer_embedding, context_embedding).item()\n",
    "\n",
    "print(f\"Faithfulness: {faithfulness_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Answer Relevancy (답변 관련성)\n",
    "- 생성된 답변이 사용자의 질문에 얼마나 관련성이 있는지를 평가합니다.\n",
    "- 질문과 생성된 답변 간의 의미적 유사도를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevancy: 0.6988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "answer_embedding = model.encode(generated_answer, convert_to_tensor=True)\n",
    "answer_relevancy_score = util.pytorch_cos_sim(question_embedding, answer_embedding).item()\n",
    "\n",
    "print(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Context Relevancy (문맥 관련성)\n",
    "- 검색된 문맥이 사용자의 질문과 얼마나 관련성이 있는지를 평가합니다.\n",
    "- 질문과 검색된 각 문장 간의 평균 유사도를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Relevancy: 0.4563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "context_sentence_embeddings = model.encode(context_sentences, convert_to_tensor=True)\n",
    "relevancy_scores = [util.pytorch_cos_sim(question_embedding, s_emb).item() for s_emb in context_sentence_embeddings]\n",
    "context_relevancy_score = np.mean(relevancy_scores)\n",
    "\n",
    "print(f\"Context Relevancy: {context_relevancy_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Context Precision (문맥 정확성)\n",
    "- 검색된 문맥 중 실제로 답변 생성에 기여한 문장의 비율을 나타냅니다.\n",
    "- 여기서는 답변과 각 문맥 문장 간의 유사도가 특정 임계값(예: 0.5)을 넘으면 기여했다고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Precision: 0.5000 (2 / 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_embedding = model.encode(generated_answer, convert_to_tensor=True)\n",
    "context_sentence_embeddings = model.encode(context_sentences, convert_to_tensor=True)\n",
    "precision_scores = [util.pytorch_cos_sim(answer_embedding, s_emb).item() for s_emb in context_sentence_embeddings]\n",
    "\n",
    "# 유사도가 0.5 이상인 문장을 '사용한' 문장으로 간주\n",
    "used_sentences = sum(1 for score in precision_scores if score > 0.5)\n",
    "context_precision_score = used_sentences / len(context_sentences) if len(context_sentences) > 0 else 0\n",
    "\n",
    "print(f\"Context Precision: {context_precision_score:.4f} ({used_sentences} / {len(context_sentences)})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Context Recall (문맥 재현율)\n",
    "- 답변 생성에 필요한 전체 정보 중 RAG가 얼마나 찾아냈는지를 평가합니다.\n",
    "- 여기서는 'ideal_context'에 필요한 정보가 모두 담겨있다고 가정하고,\n",
    "- 검색된 문맥(retrieved_context)이 이 정보를 얼마나 포함하는지 단어 단위로 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Recall: 0.6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ideal_tokens = get_word_tokens(ideal_context)\n",
    "retrieved_tokens = get_word_tokens(retrieved_context)\n",
    "retrieved_ideal_tokens = ideal_tokens.intersection(retrieved_tokens)\n",
    "context_recall_score = len(retrieved_ideal_tokens) / len(ideal_tokens) if len(ideal_tokens) > 0 else 0\n",
    "\n",
    "print(f\"Context Recall: {context_recall_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Answer Semantic Similarity (답변 의미 유사도)\n",
    "- 생성된 답변이 정답(Ground Truth)과 의미적으로 얼마나 유사한지를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "HdPJHkiiS_zm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Semantic Similarity: 0.8835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_embedding = model.encode(generated_answer, convert_to_tensor=True)\n",
    "ground_truth_embedding = model.encode(ground_truth_answer, convert_to_tensor=True)\n",
    "semantic_similarity_score = util.pytorch_cos_sim(answer_embedding, ground_truth_embedding).item()\n",
    "\n",
    "print(f\"Answer Semantic Similarity: {semantic_similarity_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Answer Correctness (답변 정확성)\n",
    "- 생성된 답변이 사실적으로 올바른지를 평가합니다.\n",
    "- 여기서는 정답에 포함된 핵심 단어가 생성된 답변에도 포함되어 있는지로 간단히 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Correctness: 1.0000 (핵심 명사 3개 중 3개 포함)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석기 로드\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "def get_nouns(text):\n",
    "    \"\"\"\n",
    "    형태소 분석기를 사용하여 텍스트에서 명사만 추출\n",
    "    \"\"\"\n",
    "    return set(okt.nouns(text))\n",
    "\n",
    "# 형태소 분석 기반의 Answer Correctness 계산 로직\n",
    "generated_answer = \"대한민국의 수도는 서울입니다.\"\n",
    "key_nouns = {'대한민국', '수도', '서울'}\n",
    "\n",
    "# 생성된 답변에서 명사 추출\n",
    "generated_answer_nouns = get_nouns(generated_answer)\n",
    "\n",
    "contained_key_nouns = key_nouns.intersection(generated_answer_nouns)\n",
    "correctness_score = len(contained_key_nouns) / len(key_nouns) if len(key_nouns) > 0 else 0\n",
    "\n",
    "print(f\"Answer Correctness: {correctness_score:.4f} (핵심 명사 {len(key_nouns)}개 중 {len(contained_key_nouns)}개 포함)\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
